# ASearcher é¡¹ç›®æ·±åº¦å­¦ä¹ æŒ‡å—

æœ¬æ–‡æ¡£æä¾›äº†ä¸€ä¸ªç³»ç»ŸåŒ–çš„ ASearcher é¡¹ç›®å­¦ä¹ è·¯å¾„ï¼Œé€šè¿‡æ¨¡å—åŒ–åˆ†ææ–¹æ³•å¸®åŠ©å¼€å‘è€…æ·±å…¥ç†è§£é¡¹ç›®æ¶æ„ã€æ ¸å¿ƒç»„ä»¶å®ç°ç»†èŠ‚ã€æ•°æ®æµæœºåˆ¶å’Œè‡ªå®šä¹‰å¼€å‘æ–¹æ³•ã€‚

## ğŸ“š ç›®å½•

1. [é¡¹ç›®æ¦‚è§ˆ](#1-é¡¹ç›®æ¦‚è§ˆ)
2. [å¿«é€Ÿå…¥é—¨è·¯å¾„](#2-å¿«é€Ÿå…¥é—¨è·¯å¾„)
3. [é¡¹ç›®æ¨¡å—æ¶æ„](#3-é¡¹ç›®æ¨¡å—æ¶æ„)
4. [æ ¸å¿ƒæ¨¡å—è¯¦è§£](#4-æ ¸å¿ƒæ¨¡å—è¯¦è§£)
   - [Module 1: Agent Core](#module-1-agent-core-æ™ºèƒ½ä½“æ ¸å¿ƒ)
   - [Module 2: Training Framework](#module-2-training-framework-è®­ç»ƒæ¡†æ¶)
   - [Module 3: Evaluation System](#module-3-evaluation-system-è¯„ä¼°ç³»ç»Ÿ)
5. [æ•°æ®æµä¸äº¤äº’æœºåˆ¶](#5-æ•°æ®æµä¸äº¤äº’æœºåˆ¶)
6. [è‡ªå®šä¹‰å¼€å‘æŒ‡å—](#6-è‡ªå®šä¹‰å¼€å‘æŒ‡å—)
7. [å­¦ä¹ èµ„æºä¸å®è·µ](#7-å­¦ä¹ èµ„æºä¸å®è·µ)

---

## 1. é¡¹ç›®æ¦‚è§ˆ

ASearcher æ˜¯ä¸€ä¸ªå¼€æºçš„å¤§è§„æ¨¡åœ¨çº¿å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è®­ç»ƒæ¡†æ¶ï¼Œä¸“ä¸ºæœç´¢æ™ºèƒ½ä½“è®¾è®¡ã€‚

### æ ¸å¿ƒåˆ›æ–°ç‚¹

- ğŸ” **æ•°æ®åˆæˆ Agent**: è‡ªåŠ¨ç”Ÿæˆé«˜è´¨é‡ã€æœ‰æŒ‘æˆ˜æ€§çš„ QA å¯¹
- âš¡ **å®Œå…¨å¼‚æ­¥çš„ Agentic RL**: è§£è€¦è½¨è¿¹æ”¶é›†ä¸æ¨¡å‹è®­ç»ƒï¼Œæ¶ˆé™¤ GPU ç©ºé—²æ—¶é—´
- ğŸŒ **é•¿è½¨è¿¹æœç´¢èƒ½åŠ›**: æ”¯æŒè¶…è¿‡ 40 è½®å·¥å…·è°ƒç”¨å’Œ 150k tokens ç”Ÿæˆ
- ğŸ† **SOTA æ€§èƒ½**: åœ¨ GAIAã€xBench-DeepSearch ç­‰åŸºå‡†ä¸Šè¾¾åˆ°é¢†å…ˆæ°´å¹³
- ğŸ“ˆ **æ˜¾è‘—çš„ RL æå‡**: é€šè¿‡ RL è®­ç»ƒå¸¦æ¥ +9.1 åˆ° +13.4 çš„æ€§èƒ½æå‡

### æ ¸å¿ƒè®¾è®¡ç†å¿µ

1. **å¼‚æ­¥æ¶æ„ä¼˜åŠ¿**ï¼šä¼ ç»ŸåŒæ­¥ RL éœ€è¦ç­‰å¾…æœ€é•¿è½¨è¿¹å®Œæˆï¼ŒGPU åˆ©ç”¨ç‡ä»… 40-60%ï¼›å®Œå…¨å¼‚æ­¥æ¶æ„å°† GPU åˆ©ç”¨ç‡æå‡è‡³ 85-95%
2. **æ¨¡å—åŒ–è®¾è®¡**ï¼šAgentã€Trainingã€Evaluation ä¸‰å¤§æ¨¡å—æ¾è€¦åˆï¼Œä¾¿äºç‹¬ç«‹å¼€å‘å’Œæµ‹è¯•
3. **å·¥å…·è°ƒç”¨ç»Ÿä¸€æ¥å£**ï¼šé€šè¿‡ XML æ ‡ç­¾ï¼ˆ`<search>`, `<access>`, `<answer>`ï¼‰ç»Ÿä¸€å·¥å…·è°ƒç”¨æ¥å£

---

## 2. å¿«é€Ÿå…¥é—¨è·¯å¾„

### ç¬¬ä¸€é˜¶æ®µï¼šç¯å¢ƒæ­å»ºä¸åˆæ¬¡è¿è¡Œï¼ˆ1-2å¤©ï¼‰

```bash
# 1. å…‹éš†é¡¹ç›®
git clone https://github.com/inclusionAI/ASearcher.git
cd ASearcher

# 2. å®‰è£…ä¾èµ–
cd AReaL
pip install -e .
pre-commit install  # å®‰è£…ä»£ç æ ¼å¼åŒ–å·¥å…·

# 3. é…ç½®å¿…è¦çš„ API Keys
export SERPER_API_KEY="your_serper_key"  # ç”¨äºç½‘é¡µæœç´¢
export JINA_API_KEY="your_jina_key"      # ç”¨äºå†…å®¹æå–

# 4. ä¸‹è½½æµ‹è¯•æ•°æ®
# ä» https://huggingface.co/datasets/inclusionAI/ASearcher-test-data ä¸‹è½½

# 5. è¿è¡Œç¬¬ä¸€ä¸ªè¯„ä¼°
cd ../evaluation
python3 search_eval_async.py \
    --data_names GAIA \
    --model_name_or_path <model_path> \
    --agent-type asearcher \
    --search-client-type async-web-search-access \
    --tensor_parallel_size 1 \
    --pass-at-k 1
```

### ç¬¬äºŒé˜¶æ®µï¼šç†è§£æ ¸å¿ƒæ¦‚å¿µï¼ˆ2-3å¤©ï¼‰

å¿…è¯»æ–‡æ¡£ï¼š
- `README.md` - é¡¹ç›®æ¦‚è¿°å’Œä¸»è¦åŠŸèƒ½
- `docs/evaluation.md` - è¯„ä¼°æµç¨‹è¯¦è§£
- `docs/training.md` - è®­ç»ƒæ–¹æ³•è¯´æ˜
- `CLAUDE.md` - å¿«é€Ÿå‚è€ƒæ‰‹å†Œ

å…³é”®æ¦‚å¿µç†è§£ï¼š
1. **å¼‚æ­¥ RL è®­ç»ƒ**ï¼šå¦‚ä½•è§£å†³å˜é•¿è½¨è¿¹çš„ GPU ç©ºé—²é—®é¢˜
2. **å¤šè½®å·¥å…·è°ƒç”¨**ï¼šAgent å¦‚ä½•è¿›è¡Œé•¿è¾¾ 128 è½®çš„æœç´¢å¯¹è¯
3. **æ•°æ®åˆæˆ**ï¼šå¦‚ä½•è‡ªåŠ¨ç”Ÿæˆé«˜è´¨é‡çš„è®­ç»ƒæ•°æ®

### ç¬¬ä¸‰é˜¶æ®µï¼šæ·±å…¥ä»£ç å®ç°ï¼ˆ3-5å¤©ï¼‰

æ¨èå­¦ä¹ é¡ºåºï¼š
1. **Agent Core** â†’ ç†è§£æ™ºèƒ½ä½“è¡Œä¸ºå’ŒçŠ¶æ€ç®¡ç†
2. **Evaluation System** â†’ ç†è§£ç«¯åˆ°ç«¯çš„æ‰§è¡Œæµç¨‹
3. **Training Framework** â†’ ç†è§£ RL è®­ç»ƒæœºåˆ¶

---

## 3. é¡¹ç›®æ¨¡å—æ¶æ„

### æ•´ä½“æ¶æ„å›¾

```
ASearcher é¡¹ç›®ç»“æ„
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ASearcher Project                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚           Module 1: Agent Core                    â”‚ â”‚
â”‚  â”‚    æ™ºèƒ½ä½“å®ç°ä¸æœç´¢å·¥å…·                            â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚         Module 2: Training Framework              â”‚ â”‚
â”‚  â”‚    AReaL å¼‚æ­¥ RL è®­ç»ƒæ¡†æ¶                         â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚          Module 3: Evaluation System              â”‚ â”‚
â”‚  â”‚    è¯„ä¼°ã€æµ‹è¯•ä¸æ€§èƒ½åˆ†æ                            â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚        Module 4: Data & QA Synthesis              â”‚ â”‚
â”‚  â”‚    æ•°æ®åˆæˆä¸å¤„ç†                                  â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### æ¨¡å—æ–‡ä»¶æ˜ å°„è¡¨

| æ¨¡å— | æ ¸å¿ƒæ–‡ä»¶ | åŠŸèƒ½è¯´æ˜ |
|-----|---------|---------|
| **Module 1: Agent Core** | | æ™ºèƒ½ä½“æ ¸å¿ƒé€»è¾‘å’Œå·¥å…·è°ƒç”¨ |
| | `agent/asearcher.py` | åŸºç¡€ Agent ç±»ï¼ŒåŒ…å« Memory å’ŒçŠ¶æ€ç®¡ç† |
| | `agent/asearcher_reasoning.py` | æ¨ç†å¢å¼ºç‰ˆ Agent |
| | `agent/search_r1.py` | Search-R1 Agent å®ç° |
| | `tools/search_utils.py` | æœç´¢å®¢æˆ·ç«¯å°è£…ï¼ˆSerperã€Jinaï¼‰ |
| | `tools/web_browser.py` | ç½‘é¡µæµè§ˆå™¨å®ç° |
| | `tools/local_retrieval_server.py` | æœ¬åœ° Wikipedia æ£€ç´¢æœåŠ¡ |
| | `AReaL/ASearcher/train/search_agent.py` | è®­ç»ƒä¸“ç”¨ Agent |
| | `AReaL/ASearcher/train/prompts.py` | æç¤ºè¯æ¨¡æ¿ |
| | `AReaL/ASearcher/utils/search_tool.py` | SearchToolBox å·¥å…·ç®±å®ç° |
| | `AReaL/ASearcher/utils/rewards.py` | å¥–åŠ±å‡½æ•°ï¼ˆF1ã€EMï¼‰ |
| **Module 2: Training Framework** | | å¼‚æ­¥ RL è®­ç»ƒåŸºç¡€è®¾æ–½ |
| | `AReaL/ASearcher/train/asearcher.py` | ASearcherWorkflow ä¸»è®­ç»ƒæµç¨‹ |
| | `AReaL/ASearcher/configs/*.yaml` | è®­ç»ƒé…ç½®æ–‡ä»¶ |
| | `AReaL/areal/workflow/multi_turn.py` | å¤šè½®å¯¹è¯å·¥ä½œæµ |
| | `AReaL/areal/engine/sglang_remote.py` | è¿œç¨‹æ¨ç†å¼•æ“ï¼ˆè´Ÿè½½å‡è¡¡ï¼‰ |
| | `AReaL/areal/engine/fsdp_engine.py` | FSDP è®­ç»ƒå¼•æ“ |
| | `AReaL/areal/engine/ppo/actor.py` | PPO Actor å®ç° |
| | `AReaL/areal/launcher/local.py` | æœ¬åœ°å¯åŠ¨å™¨ |
| | `AReaL/areal/launcher/ray.py` | Ray åˆ†å¸ƒå¼å¯åŠ¨å™¨ |
| | `AReaL/areal/launcher/slurm.py` | Slurm é›†ç¾¤å¯åŠ¨å™¨ |
| **Module 3: Evaluation System** | | æ¨¡å‹è¯„ä¼°å’Œæ€§èƒ½æµ‹è¯• |
| | `evaluation/search_eval_async.py` | å¼‚æ­¥è¯„ä¼°ä¸»ç¨‹åº |
| | `evaluation/evaluate.py` | åŒæ­¥è¯„ä¼°ç¨‹åº |
| | `evaluation/llm_as_judge.py` | LLM è¯„åˆ¤å™¨ |
| | `evaluation/llm_utils.py` | LLM å·¥å…·å‡½æ•° |
| | `evaluation/utils.py` | è¯„ä¼°æŒ‡æ ‡è®¡ç®— |
| | `evaluation/config_loader.py` | é…ç½®åŠ è½½å™¨ |
| **Module 4: Data Synthesis** | | QA å¯¹ç”Ÿæˆå’Œæ•°æ®å¤„ç† |
| | `qa_synthesis/qa_synthesis_agent.py` | QA åˆæˆ Agent |
| | `utils/index_builder.py` | ç´¢å¼•æ„å»ºå·¥å…· |

---

## 4. æ ¸å¿ƒæ¨¡å—è¯¦è§£

### Module 1: Agent Core (æ™ºèƒ½ä½“æ ¸å¿ƒ)

#### 1.1 æ ¸å¿ƒæ•°æ®ç»“æ„

```python
# agent/asearcher.py:6-21
@dataclass
class Record:
    """å¯¹è¯å†å²è®°å½•çš„åŸºæœ¬å•å…ƒ"""
    type: str              # è®°å½•ç±»å‹ï¼šprompt/llm_gen/search_results/webpage
    text: str              # å®Œæ•´æ–‡æœ¬å†…å®¹
    short_text: str = ""   # æ‘˜è¦æ–‡æœ¬ï¼ˆç”¨äº prompt æ„å»ºï¼‰
    # RL è®­ç»ƒç›¸å…³æ•°æ®
    input_len: Optional[int] = None
    input_tokens: Optional[List[int]] = None
    output_len: Optional[int] = None
    output_tokens: Optional[List[int]] = None
    output_logprobs: Optional[List[float]] = None
    output_versions: Optional[List[int]] = None
```

**å…³é”®æ´å¯Ÿ**ï¼šRecord æ•°æ®ç»“æ„æ˜¯è·¨æ¨¡å—æ•°æ®ä¼ é€’çš„æ ¸å¿ƒï¼Œæ—¢è®°å½•å¯¹è¯å†å²ï¼Œåˆæºå¸¦ RL è®­ç»ƒæ‰€éœ€çš„ token å’Œæ¦‚ç‡ä¿¡æ¯ã€‚

#### 1.2 AgentMemory è®°å¿†ç®¡ç†ç³»ç»Ÿ

```python
# agent/asearcher.py:23-68
class AgentMemory:
    """ç®¡ç†å¯¹è¯å†å²å’Œå·¥å…·äº¤äº’è®°å½•"""
    
    def __init__(self, prompt):
        self.memory = [Record(type="prompt", text=prompt)]
    
    def prepare_prompt(self) -> str:
        """æ„å»ºåŒ…å«å®Œæ•´å†å²çš„ prompt"""
        prompt = ""
        for r in self.memory:
            if r.type == "prompt":
                prompt = r.text
            elif r.type in ["search_results", "webpage"]:
                # å·¥å…·ç»“æœåæ·»åŠ æ€è€ƒæ ‡è®°
                prompt = prompt + "\n\n" + r.short_text + "\n<think>\n"
            elif r.type == "llm_gen":
                prompt = prompt + r.text
        return prompt
    
    def logging_stats(self) -> Dict:
        """ç»Ÿè®¡äº¤äº’ä¿¡æ¯"""
        return {
            "num_llm_gens": ç”Ÿæˆæ¬¡æ•°,
            "num_input_tokens": è¾“å…¥ token æ€»æ•°,
            "num_output_tokens": è¾“å‡º token æ€»æ•°,
            "num_search_queries": æœç´¢æ¬¡æ•°,
            "num_success_search_queries": æˆåŠŸæœç´¢æ•°,
            "num_pages": è®¿é—®ç½‘é¡µæ•°
        }
```

#### 1.3 AsearcherAgent ä¸»ç±»

```python
# agent/asearcher.py:69-180
class AsearcherAgent:
    """æœç´¢æ™ºèƒ½ä½“ä¸»ç±»"""
    
    def __init__(self, prompt=None):
        self.memory = AgentMemory(prompt) if prompt else None
        self.job_queue = queue.Queue(128)  # å¼‚æ­¥ä»»åŠ¡é˜Ÿåˆ—
        self.max_turns = 64                # æœ€å¤§å¯¹è¯è½®æ•°
    
    @property
    def is_finished(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦å·²ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ"""
        pattern = r'<answer>(.*?)</answer>'
        return any([re.findall(pattern, r.text) for r in self.memory.filter_records("llm_gen")])
    
    def prepare_llm_query(self) -> Tuple[str, Dict]:
        """å‡†å¤‡ LLM æŸ¥è¯¢"""
        prompt = self.memory.prepare_prompt()
        sampling_params = dict(stop=["</search>", "</access>", "</answer>"])
        
        # å¤„ç† job_queue ä¸­çš„å¾…å¤„ç†å·¥å…·å“åº”
        if not self.job_queue.empty():
            job = self.job_queue.get_nowait()
            prompt = prompt + "\n\n" + job["text"] + "\n<think>\n"
            self.memory.add_record(Record(type=job["type"], text=job["text"]))
            sampling_params["stop"] = ["</think>"]
            
        return prompt, sampling_params
    
    def consume_llm_response(self, resp, completion_text) -> List[str]:
        """è§£æ LLM å“åº”ï¼Œæå–å·¥å…·è°ƒç”¨"""
        tool_calls = []
        patterns = [
            r'<search>(.*?)</search>',   # æœç´¢å·¥å…·
            r'<access>(.*?)</access>',   # è®¿é—®ç½‘é¡µ
            r'<answer>(.*?)</answer>'    # æœ€ç»ˆç­”æ¡ˆ
        ]
        for pattern in patterns:
            matches = re.findall(pattern, completion_text, re.DOTALL)
            if matches:
                tool_calls.append(matches[-1])
        return tool_calls
```

**å…³é”®è®¾è®¡**ï¼š
1. **job_queue æœºåˆ¶**ï¼šå¼‚æ­¥å¤„ç†å·¥å…·å“åº”ï¼Œé¿å…é˜»å¡ä¸»æµç¨‹
2. **XML æ ‡ç­¾è§¦å‘**ï¼šç»Ÿä¸€çš„å·¥å…·è°ƒç”¨æ¥å£
3. **çŠ¶æ€ç®¡ç†**ï¼šé€šè¿‡ memory å’Œ is_finished ç®¡ç†å¯¹è¯çŠ¶æ€

#### 1.4 SearchToolBox å·¥å…·ç®±ç³»ç»Ÿ

```python
# AReaL/ASearcher/utils/search_tool.py:25-150
class SearchToolBox:
    """å·¥å…·è°ƒç”¨å’Œå¥–åŠ±è®¡ç®—çš„æ ¸å¿ƒç»„ä»¶"""
    
    def __init__(self, dataset_path: str, reward_type: str = "F1", 
                 topk: int = 10, search_client_type: str = "async-online-search-access"):
        self.id2info = load_metadata(dataset_path)  # åŠ è½½æ•°æ®é›†å…ƒä¿¡æ¯
        self.reward_type = reward_type              # F1 æˆ– EM
        self.topk = topk                           # æœç´¢ç»“æœæ•°é‡
        self.search_client = make_search_client(search_client_type)
    
    async def step(self, qid_actions: Tuple[str, List[str]]) -> List[Dict]:
        """æ‰§è¡Œå·¥å…·è°ƒç”¨å¹¶è®¡ç®—å¥–åŠ±"""
        qid, actions = qid_actions
        results = []
        
        for action in actions:
            result = dict(documents=None, score=None, ground_truth=None, type=None)
            
            # å¤„ç†æœç´¢å·¥å…·è°ƒç”¨
            if "<search>" in action and "</search>" in action:
                query = extract_between_tags(action, "search")
                response = await self.search_client.query_async({
                    "queries": [query],
                    "topk": self.topk
                })
                result["documents"] = response[0]["documents"]
                result["urls"] = response[0]["urls"]
                result["type"] = "search"
            
            # å¤„ç†ç½‘é¡µè®¿é—®
            elif "<access>" in action and "</access>" in action:
                url = extract_between_tags(action, "access")
                response = await self.search_client.access_async([url])
                result["page"] = process_webpage(response[0].get("page", ""))
                result["type"] = "access"
            
            # è®¡ç®—å¥–åŠ±
            ground_truth = self.id2info[qid]["answer"]
            if self.reward_type == "F1":
                extracted, score = compute_score_f1(action, ground_truth)
            elif self.reward_type == "EM":
                extracted, score = compute_score_em(action, ground_truth)
            
            result["score"] = score
            result["ground_truth"] = ground_truth
            results.append(result)
            
        return results
```

### Module 2: Training Framework (è®­ç»ƒæ¡†æ¶)

#### 2.1 ASearcherWorkflow è®­ç»ƒå·¥ä½œæµ

```python
# AReaL/ASearcher/train/asearcher.py:50-220
class ASearcherWorkflow(RolloutWorkflow):
    """å¼‚æ­¥ RL è®­ç»ƒçš„æ ¸å¿ƒå·¥ä½œæµ"""
    
    def __init__(self, gconfig: GenerationHyperparameters, tokenizer: PreTrainedTokenizerFast,
                 dataset_path: str, dump_dir: str = None, max_turns: int = 128,
                 n_trajs: int = 1, search_client_type: str = "async-online-search-access",
                 reward_type: str = "F1", topk: int = 5, valid_inst_ratio: float = 1.0,
                 max_tokens: int = 32000, search_only: bool = True):
        """
        æ ¸å¿ƒå‚æ•°:
        - max_turns: 128        # æœ€å¤§å¯¹è¯è½®æ•°ï¼ˆæ”¯æŒæé•¿æœç´¢ï¼‰
        - n_trajs: 1           # æ¯ä¸ªé—®é¢˜æ”¶é›†çš„è½¨è¿¹æ•°
        - reward_type: "F1"    # å¥–åŠ±ç±»å‹ï¼ˆF1 æˆ– EMï¼‰
        - search_client_type   # æœç´¢å®¢æˆ·ç«¯ç±»å‹
        - valid_inst_ratio     # æœ‰æ•ˆå®ä¾‹æ¯”ä¾‹ï¼ˆç”¨äºè¯¾ç¨‹å­¦ä¹ ï¼‰
        """
        self.toolbox = SearchToolBox(dataset_path, reward_type, topk, search_client_type)
    
    async def collect_agent_trajectory(self, valid_inst, qid, prompt, engine):
        """æ”¶é›†å•æ¡è½¨è¿¹ï¼ˆæ ¸å¿ƒå¼‚æ­¥æµç¨‹ï¼‰"""
        agent = SearchAgent(prompt)
        score = 0
        ground_truth = None
        traj_rid = uuid.uuid4().hex  # è½¨è¿¹å”¯ä¸€ IDï¼Œç¡®ä¿è·¯ç”±åˆ°åŒä¸€æœåŠ¡å™¨
        
        while agent.num_turns < self.max_turns and not agent.is_finished:
            # 1. å‡†å¤‡ LLM æŸ¥è¯¢
            query_prompt, sampling_params = agent.prepare_llm_query()
            
            # 2. å¼‚æ­¥ LLM ç”Ÿæˆ
            input_ids = self.tokenizer.encode(query_prompt)
            req = LLMRequest(rid=traj_rid, input_ids=input_ids, gconfig=self.gconfig)
            resp = await engine.agenerate(req)  # å¼‚æ­¥ç”Ÿæˆ
            completion_str = self.tokenizer.decode(resp.output_tokens)
            
            # 3. æå–å·¥å…·è°ƒç”¨
            tool_calls = agent.consume_llm_response(resp, completion_str)
            
            # 4. å¼‚æ­¥æ‰§è¡Œå·¥å…·å¹¶è®¡ç®—å¥–åŠ±
            if tool_calls:
                res = (await self.toolbox.step((qid, tool_calls)))[0]
                agent.consume_tool_response(res, topk=self.topk)
                
                if "score" in res:
                    score = res["score"]
                if "ground_truth" in res:
                    ground_truth = res["ground_truth"]
            
            # 5. æ£€æŸ¥ç»ˆæ­¢æ¡ä»¶
            if resp.output_tokens[-1] in [tokenizer.eos_token_id]:
                break
        
        # 6. è®¡ç®—æœ€ç»ˆå¥–åŠ±
        format_reward = float(all([correct_format_fn(i, r.text) 
                                   for i, r in enumerate(agent.memory.filter_records("llm_gen"))]))
        score = score * format_reward
        
        # 7. å¤„ç†æ— æ•ˆé—®é¢˜åˆ¤æ–­
        if valid_inst and judge_q_invalid:
            score = -0.5  # æƒ©ç½šåˆ¤æ–­é”™è¯¯
        
        return ground_truth, score, agent.memory, stats
    
    async def arun_episode(self, engine, data):
        """è¿è¡Œä¸€ä¸ªè®­ç»ƒå›åˆ"""
        # 1. å‡†å¤‡ prompt
        prompt_template = SEARCH_ONLY_PROMPT_TEMPLATE if self.search_only else SEARCH_ACCESS_PROMPT_TEMPLATE
        prompt = prompt_template.format(question=data["question"])
        
        # 2. å¹¶å‘æ”¶é›†å¤šæ¡è½¨è¿¹
        trajs = await asyncio.gather(*[
            self.collect_agent_trajectory(valid_inst, qid, prompt, engine) 
            for _ in range(self.n_trajs)
        ])
        
        # 3. å½’ä¸€åŒ–å¥–åŠ±ï¼ˆå‡å»å‡å€¼ï¼‰
        scores = [score for _, score, _, _ in trajs]
        score_mean = np.mean(scores)
        normalized_scores = [s - score_mean for s in scores]
        
        # 4. è½¬æ¢ä¸ºè®­ç»ƒæ•°æ®
        results = []
        for i, (_, score, memory, stats) in enumerate(trajs):
            for record in memory.memory:
                if record.type == "llm_gen":
                    results.append({
                        "input_ids": torch.tensor(record.input_tokens + record.output_tokens),
                        "logprobs": torch.tensor([0.0] * len(record.input_tokens) + record.output_logprobs),
                        "loss_mask": torch.tensor([0] * len(record.input_tokens) + [1] * len(record.output_tokens)),
                        "rewards": torch.tensor(normalized_scores[i]),
                        "versions": torch.tensor(record.output_versions)
                    })
        
        return results
```

#### 2.2 RemoteSGLangEngine æ¨ç†å¼•æ“

```python
# AReaL/areal/engine/sglang_remote.py:36-200
class RemoteSGLangEngine(InferenceEngine):
    """è¿œç¨‹æ¨ç†å¼•æ“ï¼Œæ”¯æŒè´Ÿè½½å‡è¡¡"""
    
    def __init__(self, config: InferenceEngineConfig):
        # RID ç¼“å­˜æœºåˆ¶ï¼ˆç²˜æ€§è·¯ç”±ï¼‰
        self.rid_to_address = {}     # RID â†’ æœåŠ¡å™¨åœ°å€æ˜ å°„
        self.rid_queue = []          # LRU ç¼“å­˜é˜Ÿåˆ—ï¼ˆæœ€å¤š 128 ä¸ªï¼‰
        
        # æœåŠ¡å™¨åˆ—è¡¨
        self.addresses = os.getenv("AREAL_LLM_SERVER_ADDRS").split(",")
        self.server_idx = random.randint(0, len(self.addresses) - 1)
    
    def choose_server(self, rid: str) -> str:
        """é€‰æ‹©æœåŠ¡å™¨ï¼ˆè´Ÿè½½å‡è¡¡ç­–ç•¥ï¼‰"""
        if self.config.schedule_policy == "round_robin":
            # è½®è¯¢ç­–ç•¥
            self.server_idx = (self.server_idx + 1) % len(self.addresses)
            return self.addresses[self.server_idx]
            
        elif self.config.schedule_policy == "sticky":
            # ç²˜æ€§è·¯ç”±ï¼ˆåŒä¸€è½¨è¿¹è·¯ç”±åˆ°åŒä¸€æœåŠ¡å™¨ï¼‰
            if rid in self.rid_to_address:
                return self.rid_to_address[rid]
            else:
                # é€‰æ‹©æ–°æœåŠ¡å™¨å¹¶ç¼“å­˜
                addr = self.addresses[self.server_idx]
                self.rid_to_address[rid] = addr
                
                # LRU ç¼“å­˜ç®¡ç†
                self.rid_queue.append(rid)
                if len(self.rid_queue) > RID_CACHE_SIZE:
                    old_rid = self.rid_queue.pop(0)
                    del self.rid_to_address[old_rid]
                
                self.server_idx = (self.server_idx + 1) % len(self.addresses)
                return addr
    
    async def agenerate(self, req: LLMRequest) -> LLMResponse:
        """å¼‚æ­¥ç”Ÿæˆï¼ˆæ ¸å¿ƒæ¥å£ï¼‰"""
        # 1. é€‰æ‹©æœåŠ¡å™¨
        addr = self.choose_server(req.rid)
        
        # 2. æ„å»ºè¯·æ±‚
        payload = {
            "text": self.tokenizer.decode(req.input_ids),
            "sampling_params": {
                "temperature": req.gconfig.temperature,
                "top_p": req.gconfig.top_p,
                "max_new_tokens": req.gconfig.max_new_tokens,
                "stop": req.gconfig.stop
            }
        }
        
        # 3. å¼‚æ­¥ HTTP è¯·æ±‚
        async with aiohttp.ClientSession() as session:
            resp = await arequest_with_retry(
                session=session,
                url=f"http://{addr}/generate",
                json=payload,
                max_retries=3,
                timeout=120
            )
        
        # 4. æ„å»ºå“åº”
        return LLMResponse(
            input_tokens=req.input_ids,
            output_tokens=self.tokenizer.encode(resp["text"]),
            output_logprobs=resp.get("logprobs", []),
            output_versions=[self._version] * len(resp["text"])
        )
```

#### 2.3 PPOActor PPO è®­ç»ƒå™¨

```python
# AReaL/areal/engine/ppo/actor.py:20-150
class PPOActor:
    """PPO è®­ç»ƒçš„æ ¸å¿ƒå®ç°"""
    
    def __init__(self, config: PPOActorConfig, engine: TrainEngine):
        # å¥–åŠ±å¤„ç†å‚æ•°
        self.reward_bias = config.reward_bias           # å¥–åŠ±åç½®
        self.reward_scaling = config.reward_scaling     # å¥–åŠ±ç¼©æ”¾
        self.reward_clip = config.reward_clip           # å¥–åŠ±è£å‰ª
        
        # å½’ä¸€åŒ–å‚æ•°
        self.group_reward_norm = config.group_reward_norm  # ç»„å†…å½’ä¸€åŒ–
        self.group_adv_norm = config.group_adv_norm       # ä¼˜åŠ¿å½’ä¸€åŒ–
        self.group_size = config.group_size               # ç»„å¤§å°
        
        # PPO å‚æ•°
        self.kl_ctl = config.kl_ctl        # KL æ•£åº¦ç³»æ•°
        self.discount = config.discount     # æŠ˜æ‰£å› å­
        self.gae_lambda = config.gae_lambda # GAE Î» å‚æ•°
    
    def compute_advantages(self, data: TensorDict) -> None:
        """è®¡ç®—ä¼˜åŠ¿å‡½æ•°ï¼ˆGAEï¼‰"""
        bs = data["input_ids"].shape[0]
        
        # 1. å¥–åŠ±å¤„ç†
        reward_score = data["rewards"]
        reward_score = (reward_score + self.reward_bias) * self.reward_scaling
        reward_score = torch.clip(reward_score, max=self.reward_clip, min=-self.reward_clip)
        
        # 2. ç»„å†…å½’ä¸€åŒ–ï¼ˆå¯é€‰ï¼‰
        if self.group_reward_norm:
            for i in range(bs // self.group_size):
                s = slice(i * self.group_size, (i + 1) * self.group_size)
                r = reward_score[s]
                reward_score[s] = (r - r.mean()) / (r.std() + 1e-9)
        
        # 3. è®¡ç®— GAE (Generalized Advantage Estimation)
        advantages = []
        returns = []
        gae = 0
        
        for t in reversed(range(len(rewards))):
            if t == len(rewards) - 1:
                next_value = 0  # Bootstrap value
            else:
                next_value = values[t + 1]
            
            # TD è¯¯å·®
            delta = rewards[t] + self.discount * next_value - values[t]
            
            # GAE ç´¯ç§¯
            gae = delta + self.discount * self.gae_lambda * gae
            advantages.insert(0, gae)
            returns.insert(0, gae + values[t])
        
        # 4. ä¼˜åŠ¿å½’ä¸€åŒ–
        if self.adv_norm:
            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-9)
        
        data["advantages"] = torch.tensor(advantages)
        data["returns"] = torch.tensor(returns)
    
    def compute_loss(self, data: TensorDict) -> torch.Tensor:
        """è®¡ç®— PPO æŸå¤±"""
        # 1. è®¡ç®—æ¦‚ç‡æ¯”ç‡
        old_logprobs = data["old_logprobs"]
        new_logprobs = self.compute_logp(data)
        ratio = torch.exp(new_logprobs - old_logprobs)
        
        # 2. Policy Loss (Clipped Surrogate Objective)
        advantages = data["advantages"]
        surr1 = ratio * advantages
        surr2 = torch.clamp(ratio, 1 - self.clip_epsilon, 1 + self.clip_epsilon) * advantages
        policy_loss = -torch.min(surr1, surr2).mean()
        
        # 3. Value Loss
        value_pred = self.engine.forward_value(data)
        value_target = data["returns"]
        value_loss = F.mse_loss(value_pred, value_target)
        
        # 4. KL Penalty (ç”¨äºç¨³å®šè®­ç»ƒ)
        kl_div = (old_logprobs - new_logprobs).mean()
        kl_penalty = self.kl_ctl * kl_div
        
        # 5. æ€»æŸå¤±
        total_loss = policy_loss + self.value_coef * value_loss + kl_penalty
        
        return total_loss
```

#### 2.4 åˆ†å¸ƒå¼å¯åŠ¨å™¨

```python
# æœ¬åœ°å¯åŠ¨å™¨ - areal/launcher/local.py
class LocalLauncher:
    """å•èŠ‚ç‚¹å¤š GPU è®­ç»ƒ"""
    def launch(self, script_path: str, config: Dict):
        # ä½¿ç”¨ torchrun å¯åŠ¨
        cmd = [
            "torchrun",
            f"--nproc_per_node={config['n_gpus_per_node']}",
            script_path,
            "--config", config_path
        ]
        subprocess.run(cmd)

# Ray å¯åŠ¨å™¨ - areal/launcher/ray.py
class RayLauncher:
    """å¤šèŠ‚ç‚¹åˆ†å¸ƒå¼è®­ç»ƒ"""
    def launch(self, script_path: str, config: Dict):
        ray.init(address=config['ray_address'])
        
        # åˆ›å»ºè¿œç¨‹ä»»åŠ¡
        @ray.remote(num_gpus=1)
        def train_worker(rank, world_size):
            # åˆå§‹åŒ–åˆ†å¸ƒå¼ç¯å¢ƒ
            dist.init_process_group(
                backend="nccl",
                rank=rank,
                world_size=world_size
            )
            # æ‰§è¡Œè®­ç»ƒ
            exec(open(script_path).read())
        
        # å¯åŠ¨æ‰€æœ‰ worker
        futures = [
            train_worker.remote(i, config['world_size']) 
            for i in range(config['world_size'])
        ]
        ray.get(futures)

# Slurm å¯åŠ¨å™¨ - areal/launcher/slurm.py
class SlurmLauncher:
    """HPC é›†ç¾¤æ”¯æŒ"""
    def launch(self, script_path: str, config: Dict):
        sbatch_script = f"""
        #!/bin/bash
        #SBATCH --nodes={config['n_nodes']}
        #SBATCH --gpus-per-node={config['n_gpus_per_node']}
        #SBATCH --job-name={config['job_name']}
        
        srun python {script_path} --config {config_path}
        """
        
        with open("job.sbatch", "w") as f:
            f.write(sbatch_script)
        
        subprocess.run(["sbatch", "job.sbatch"])
```

### Module 3: Evaluation System (è¯„ä¼°ç³»ç»Ÿ)

#### 3.1 ä¸»è¯„ä¼°æµç¨‹

```python
# evaluation/search_eval_async.py
async def evaluate_sample(data: Dict, agent_type: str, search_client_type: str) -> Dict:
    """è¯„ä¼°å•ä¸ªæ ·æœ¬çš„æ ¸å¿ƒæµç¨‹"""
    
    # 1. åˆå§‹åŒ–ç»„ä»¶
    agent = make_agent(agent_type)  # å·¥å‚å‡½æ•°åˆ›å»º Agent
    agent.initialize_with_prompt(data["question"])
    
    search_client = make_search_client(search_client_type)  # åˆ›å»ºæœç´¢å®¢æˆ·ç«¯
    llm = get_sglang_llm(model_path, tensor_parallel_size)  # åˆå§‹åŒ– LLM
    
    # 2. æ‰§è¡Œè¯„ä¼°å¾ªç¯
    turns = 0
    while not agent.is_finished and turns < max_turns:
        # 2.1 å‡†å¤‡ LLM æŸ¥è¯¢
        prompt, sampling_params = agent.prepare_llm_query()
        
        # 2.2 LLM ç”Ÿæˆï¼ˆå¼‚æ­¥ï¼‰
        response = await llm.generate(
            prompt=prompt,
            sampling_params=sampling_params
        )
        
        # 2.3 å¤„ç† LLM å“åº”
        tool_calls = agent.consume_llm_response(
            resp=CompatibleLLMResponse(
                text=response.text,
                input_tokens=response.input_tokens,
                output_tokens=response.output_tokens,
                output_logprobs=response.logprobs
            ),
            completion_text=response.text
        )
        
        # 2.4 æ‰§è¡Œå·¥å…·è°ƒç”¨ï¼ˆå¼‚æ­¥ï¼‰
        if tool_calls:
            for tool_call in tool_calls:
                if "<search>" in tool_call:
                    query = extract_between_tags(tool_call, "search")
                    results = await search_client.search(query)
                    agent.add_jobs([{
                        "type": "search_results",
                        "text": format_search_results(results),
                        "short_text": truncate(results, max_len=500)
                    }])
                    
                elif "<access>" in tool_call:
                    url = extract_between_tags(tool_call, "access")
                    content = await search_client.access(url)
                    agent.add_jobs([{
                        "type": "webpage",
                        "text": content,
                        "short_text": truncate(content, max_len=1000)
                    }])
        
        turns += 1
    
    # 3. æå–ç­”æ¡ˆå¹¶è®¡ç®—åˆ†æ•°
    answer = extract_answer(agent.memory)
    ground_truth = data["answer"]
    
    # 3.1 è®¡ç®— F1 åˆ†æ•°
    f1_score = compute_f1(answer, ground_truth)
    
    # 3.2 è®¡ç®— EM åˆ†æ•°
    em_score = compute_em(answer, ground_truth)
    
    # 3.3 ç»Ÿè®¡ä¿¡æ¯
    stats = agent.memory.logging_stats()
    
    return {
        "question_id": data["id"],
        "predicted_answer": answer,
        "ground_truth": ground_truth,
        "f1_score": f1_score,
        "em_score": em_score,
        "num_turns": turns,
        "stats": stats
    }

async def main():
    """ä¸»è¯„ä¼°æµç¨‹"""
    # 1. åŠ è½½æ•°æ®
    test_data = prepare_data(args.data_names)  # åŠ è½½ GAIA/xBench/Frames
    
    # 2. å¹¶å‘è¯„ä¼°
    tasks = []
    for sample in test_data[:args.num_test_sample]:
        # æ”¯æŒ Pass@k è¯„ä¼°
        for k in range(args.pass_at_k):
            tasks.append(evaluate_sample(
                data=sample,
                agent_type=args.agent_type,
                search_client_type=args.search_client_type
            ))
    
    results = await asyncio.gather(*tasks)
    
    # 3. ç»“æœèšåˆ
    metrics = compute_metrics(results)
    
    # 3.1 Avg@k: å¹³å‡åˆ†æ•°
    avg_f1 = np.mean([r["f1_score"] for r in results])
    avg_em = np.mean([r["em_score"] for r in results])
    
    # 3.2 Pass@k: é€šè¿‡ç‡
    pass_at_k = compute_pass_at_k(results, k=args.pass_at_k)
    
    # 4. è¾“å‡ºç»“æœ
    print_results_table(metrics)
    save_results_json(results, args.output_dir)
```

#### 3.2 LLM è¯„åˆ¤å™¨

```python
# evaluation/llm_as_judge.py
class LLMJudge:
    """ä½¿ç”¨ LLM è¿›è¡Œå®šæ€§è¯„ä¼°"""
    
    def __init__(self, judge_model="gpt-4"):
        self.judge_model = judge_model
        
    async def judge_answer(self, question: str, predicted: str, 
                           ground_truth: str, trajectory: List[Dict]) -> Dict:
        """è¯„åˆ¤ç­”æ¡ˆè´¨é‡"""
        
        # 1. æ„å»ºè¯„åˆ¤ prompt
        judge_prompt = f"""
        You are an expert evaluator. Please evaluate the following answer.
        
        Question: {question}
        
        Ground Truth Answer: {ground_truth}
        
        Predicted Answer: {predicted}
        
        Trajectory Summary:
        - Number of searches: {count_searches(trajectory)}
        - Number of web accesses: {count_accesses(trajectory)}
        - Total turns: {len(trajectory)}
        
        Please evaluate based on:
        1. Correctness: Is the answer factually correct?
        2. Completeness: Does it fully address the question?
        3. Reasoning: Is the reasoning process sound?
        4. Efficiency: Was the search process efficient?
        
        Provide a score from 0-100 and explain your reasoning.
        """
        
        # 2. è°ƒç”¨è¯„åˆ¤æ¨¡å‹
        response = await call_judge_llm(judge_prompt, self.judge_model)
        
        # 3. è§£æè¯„åˆ¤ç»“æœ
        score = extract_score(response)
        reasoning = extract_reasoning(response)
        
        return {
            "judge_score": score,
            "judge_reasoning": reasoning,
            "correctness": check_correctness(predicted, ground_truth),
            "efficiency": len(trajectory) / max_expected_turns
        }
```

#### 3.3 è¯„ä¼°æŒ‡æ ‡è®¡ç®—

```python
# evaluation/utils.py
def compute_f1(prediction: str, ground_truth: str) -> float:
    """è®¡ç®— F1 åˆ†æ•°"""
    # 1. æ ‡å‡†åŒ–æ–‡æœ¬
    pred_tokens = normalize_answer(prediction).split()
    gold_tokens = normalize_answer(ground_truth).split()
    
    # 2. è®¡ç®—é‡å 
    common = set(pred_tokens) & set(gold_tokens)
    
    # 3. è®¡ç®—ç²¾ç¡®ç‡å’Œå¬å›ç‡
    if len(pred_tokens) == 0:
        precision = 0
    else:
        precision = len(common) / len(pred_tokens)
    
    if len(gold_tokens) == 0:
        recall = 0
    else:
        recall = len(common) / len(gold_tokens)
    
    # 4. è®¡ç®— F1
    if precision + recall == 0:
        return 0.0
    
    f1 = 2 * precision * recall / (precision + recall)
    return f1

def compute_em(prediction: str, ground_truth: str) -> float:
    """è®¡ç®—ç²¾ç¡®åŒ¹é…åˆ†æ•°"""
    return float(normalize_answer(prediction) == normalize_answer(ground_truth))

def compute_pass_at_k(results: List[Dict], k: int) -> float:
    """è®¡ç®— Pass@k æŒ‡æ ‡"""
    # æŒ‰é—®é¢˜åˆ†ç»„
    grouped = defaultdict(list)
    for r in results:
        grouped[r["question_id"]].append(r["f1_score"])
    
    # è®¡ç®—æ¯ä¸ªé—®é¢˜çš„ Pass@k
    pass_count = 0
    for qid, scores in grouped.items():
        # å–å‰ k ä¸ªç»“æœçš„æœ€é«˜åˆ†
        top_k_scores = scores[:k]
        if max(top_k_scores) >= 0.5:  # é˜ˆå€¼å¯è°ƒ
            pass_count += 1
    
    return pass_count / len(grouped)

def normalize_answer(s: str) -> str:
    """æ ‡å‡†åŒ–ç­”æ¡ˆæ–‡æœ¬"""
    # å°å†™åŒ–
    s = s.lower()
    # ç§»é™¤æ ‡ç‚¹
    s = re.sub(r'[^\w\s]', '', s)
    # ç§»é™¤å¤šä½™ç©ºæ ¼
    s = ' '.join(s.split())
    # ç§»é™¤å† è¯
    s = re.sub(r'\b(a|an|the)\b', '', s)
    return s.strip()
```

---

## 5. æ•°æ®æµä¸äº¤äº’æœºåˆ¶

### 5.1 è¯„ä¼°åœºæ™¯çš„æ•°æ®æµ

```
æ•°æ®æµè¯¦ç»†è·¯å¾„ï¼š
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

1. è¾“å…¥é˜¶æ®µ
   load_jsonl(data_path) 
      â†“
   {"question": "...", "answer": "...", "id": "..."}
      â†“
   agent.initialize_with_prompt(question)

2. æ‰§è¡Œå¾ªç¯
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  while not agent.is_finished:       â”‚
   â”‚                                     â”‚
   â”‚    prepare_llm_query()              â”‚
   â”‚         â†“                           â”‚
   â”‚    æ„å»º prompt + å†å²               â”‚
   â”‚         â†“                           â”‚
   â”‚    LLM.generate()                   â”‚
   â”‚         â†“                           â”‚
   â”‚    è§£æ XML æ ‡ç­¾                    â”‚
   â”‚         â†“                           â”‚
   â”‚    æ‰§è¡Œå·¥å…·è°ƒç”¨                     â”‚
   â”‚         â†“                           â”‚
   â”‚    æ›´æ–° agent.memory                â”‚
   â”‚         â†“                           â”‚
   â”‚    æ·»åŠ åˆ° job_queue                 â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

3. ç»“æœå¤„ç†
   extract_answer(agent.memory)
      â†“
   compute_score(answer, ground_truth)
      â†“
   aggregate_results()
```

### 5.2 è®­ç»ƒåœºæ™¯çš„æ•°æ®æµ

```
è®­ç»ƒæ•°æ®æµè¯¦ç»†è·¯å¾„ï¼š
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

1. æ•°æ®å‡†å¤‡
   load_dataset(dataset_path)
      â†“
   åˆ›å»º DataLoader
      â†“
   æ‰¹æ¬¡æ•°æ®

2. è½¨è¿¹æ”¶é›†ï¼ˆå¼‚æ­¥ï¼‰
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  async def arun_episode():           â”‚
   â”‚                                      â”‚
   â”‚    tasks = []                        â”‚
   â”‚    for _ in range(n_trajs):         â”‚
   â”‚      tasks.append(                   â”‚
   â”‚        collect_agent_trajectory()    â”‚
   â”‚      )                               â”‚
   â”‚                                      â”‚
   â”‚    trajs = await gather(*tasks)      â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

3. è½¨è¿¹å¤„ç†
   normalize_rewards(trajs)
      â†“
   convert_to_tensors()
      â†“
   TensorDict {
     "input_ids": [...],
     "logprobs": [...],
     "rewards": [...],
     "advantages": [...]
   }

4. PPO è®­ç»ƒ
   compute_advantages()
      â†“
   compute_loss()
      â†“
   backward()
      â†“
   optimizer.step()
```

### 5.3 æ¨¡å—é—´æ¥å£å¥‘çº¦

```python
# æ ¸å¿ƒæ¥å£å®šä¹‰
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# 1. Agent æ¥å£
class IAgent:
    """Agent æ ‡å‡†æ¥å£"""
    
    @property
    def is_finished(self) -> bool:
        """æ˜¯å¦å·²å®Œæˆ"""
        
    @property
    def num_turns(self) -> int:
        """å½“å‰è½®æ•°"""
    
    def prepare_llm_query(self) -> Tuple[str, Dict]:
        """å‡†å¤‡ LLM æŸ¥è¯¢"""
        
    def consume_llm_response(self, resp, text) -> List[str]:
        """å¤„ç† LLM å“åº”"""
        
    def consume_tool_response(self, results) -> None:
        """å¤„ç†å·¥å…·å“åº”"""

# 2. Tool æ¥å£
class ITool:
    """å·¥å…·æ ‡å‡†æ¥å£"""
    
    async def execute(self, action: str) -> Dict:
        """æ‰§è¡Œå·¥å…·è°ƒç”¨"""
        return {
            "type": str,         # å·¥å…·ç±»å‹
            "result": Any,       # æ‰§è¡Œç»“æœ
            "success": bool,     # æ˜¯å¦æˆåŠŸ
            "metadata": Dict     # å…ƒæ•°æ®
        }

# 3. Engine æ¥å£
class IEngine:
    """æ¨ç†å¼•æ“æ¥å£"""
    
    async def agenerate(self, request: LLMRequest) -> LLMResponse:
        """å¼‚æ­¥ç”Ÿæˆ"""
        
    def get_version(self) -> int:
        """è·å–æ¨¡å‹ç‰ˆæœ¬"""
        
    def update_weights(self, weights: Dict) -> None:
        """æ›´æ–°æ¨¡å‹æƒé‡"""

# 4. Workflow æ¥å£
class IWorkflow:
    """è®­ç»ƒå·¥ä½œæµæ¥å£"""
    
    async def arun_episode(self, engine: IEngine, data: Dict) -> TensorDict:
        """è¿è¡Œè®­ç»ƒå›åˆ"""
        
    async def collect_agent_trajectory(self, prompt: str, engine: IEngine) -> Tuple:
        """æ”¶é›†è½¨è¿¹"""
```

### 5.4 å¼‚æ­¥æ¶æ„çš„å®ç°ç»†èŠ‚

```python
# å®Œå…¨å¼‚æ­¥æ¶æ„çš„æ ¸å¿ƒä¼˜åŠ¿
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ä¼ ç»ŸåŒæ­¥æ¶æ„é—®é¢˜:
â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”
â”‚ T1  â”‚ T2  â”‚ T3  â”‚ T4  â”‚ â†’ å¿…é¡»ç­‰å¾…æœ€é•¿è½¨è¿¹
â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜
GPU ç©ºé—²æ—¶é—´: 60%

å®Œå…¨å¼‚æ­¥æ¶æ„ä¼˜åŠ¿:
â”Œâ”€â”€â”€â”€â”€â”
â”‚ T1  â”‚ â†’ 2è½®å®Œæˆ
â””â”€â”€â”€â”€â”€â”˜
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚   T2     â”‚ â†’ 10è½®å®Œæˆ
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚       T3          â”‚ â†’ 40è½®å®Œæˆ
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
GPU ç©ºé—²æ—¶é—´: 5%

# å¼‚æ­¥å®ç°å…³é”®ä»£ç 
async def collect_trajectories(n_trajs: int):
    """å¹¶å‘æ”¶é›†è½¨è¿¹"""
    # åˆ›å»ºä¿¡å·é‡æ§åˆ¶å¹¶å‘æ•°
    semaphore = asyncio.Semaphore(max_concurrent_rollouts)
    
    async def controlled_collect(prompt):
        async with semaphore:
            return await collect_agent_trajectory(prompt)
    
    # å¹¶å‘æ‰§è¡Œ
    tasks = [controlled_collect(prompt) for _ in range(n_trajs)]
    results = await asyncio.gather(*tasks)
    
    # ç«‹å³å¤„ç†å®Œæˆçš„è½¨è¿¹ï¼Œä¸ç­‰å¾…æ‰€æœ‰è½¨è¿¹
    return results

# RID ç¼“å­˜æœºåˆ¶ï¼ˆä¿è¯åŒè½¨è¿¹è·¯ç”±åˆ°åŒæœåŠ¡å™¨ï¼‰
class RIDCache:
    def __init__(self, size=128):
        self.cache = OrderedDict()
        self.size = size
    
    def get_server(self, rid: str) -> str:
        if rid in self.cache:
            # ç§»åˆ°æœ€åï¼ˆLRUï¼‰
            self.cache.move_to_end(rid)
            return self.cache[rid]
        
        # åˆ†é…æ–°æœåŠ¡å™¨
        server = self.choose_new_server()
        self.cache[rid] = server
        
        # æ·˜æ±°æœ€æ—§çš„
        if len(self.cache) > self.size:
            self.cache.popitem(last=False)
        
        return server
```

---

## 6. è‡ªå®šä¹‰å¼€å‘æŒ‡å—

### 6.1 è‡ªå®šä¹‰ Agent è¡Œä¸º

#### æ–¹æ¡ˆ A: ç»§æ‰¿ç°æœ‰ Agent

```python
# æ–‡ä»¶: agent/my_custom_agent.py

from agent.asearcher import AsearcherAgent, AgentMemory, Record
from typing import List, Tuple, Dict

class MyCustomAgent(AsearcherAgent):
    """è‡ªå®šä¹‰ Agent ç¤ºä¾‹ï¼šæ·»åŠ æ€ç»´é“¾å’Œæ¨ç†æ­¥éª¤"""
    
    def __init__(self, prompt: str):
        super().__init__(prompt)
        self.reasoning_steps = []      # å­˜å‚¨æ¨ç†æ­¥éª¤
        self.search_history = []       # æœç´¢å†å²
        self.confidence_scores = []    # ç½®ä¿¡åº¦åˆ†æ•°
        
    def prepare_llm_query(self) -> Tuple[str, Dict]:
        """è‡ªå®šä¹‰æŸ¥è¯¢å‡†å¤‡é€»è¾‘"""
        prompt, params = super().prepare_llm_query()
        
        # 1. æ·»åŠ æ€ç»´é“¾æç¤º
        if self.num_turns > 0:
            prompt += "\n\nLet's approach this step-by-step:"
            prompt += "\n1. What do we know so far?"
            prompt += "\n2. What information is still missing?"
            prompt += "\n3. What should we search for next?\n"
        
        # 2. æ·»åŠ æœç´¢å†å²ä¸Šä¸‹æ–‡
        if self.search_history:
            prompt += "\n\nPrevious searches:\n"
            for i, search in enumerate(self.search_history[-3:], 1):
                prompt += f"{i}. {search}\n"
        
        # 3. è°ƒæ•´ç”Ÿæˆå‚æ•°
        params.update({
            "temperature": 0.7 if self.num_turns < 5 else 0.5,  # é€æ¸é™ä½æ¸©åº¦
            "top_p": 0.9,
            "repetition_penalty": 1.1  # é¿å…é‡å¤
        })
        
        return prompt, params
    
    def consume_llm_response(self, resp, text: str) -> List[str]:
        """å¢å¼ºçš„å“åº”å¤„ç†"""
        # 1. æå–æ¨ç†æ­¥éª¤
        if "Step:" in text or "Reasoning:" in text:
            reasoning = self.extract_reasoning(text)
            self.reasoning_steps.append(reasoning)
        
        # 2. è®¡ç®—ç½®ä¿¡åº¦
        confidence = self.calculate_confidence(text)
        self.confidence_scores.append(confidence)
        
        # 3. è°ƒç”¨çˆ¶ç±»å¤„ç†å·¥å…·è°ƒç”¨
        tool_calls = super().consume_llm_response(resp, text)
        
        # 4. è®°å½•æœç´¢å†å²
        for call in tool_calls:
            if "<search>" in call:
                query = call.split("<search>")[1].split("</search>")[0]
                self.search_history.append(query)
        
        return tool_calls
    
    def extract_reasoning(self, text: str) -> str:
        """æå–æ¨ç†æ­¥éª¤"""
        patterns = [
            r"Step \d+:(.*?)(?=Step \d+:|$)",
            r"Reasoning:(.*?)(?=\n\n|$)",
            r"Therefore,(.*?)(?=\n|$)"
        ]
        
        for pattern in patterns:
            matches = re.findall(pattern, text, re.DOTALL)
            if matches:
                return matches[0].strip()
        
        return ""
    
    def calculate_confidence(self, text: str) -> float:
        """è®¡ç®—å›ç­”ç½®ä¿¡åº¦"""
        confidence_indicators = {
            "certain": 0.9,
            "likely": 0.7,
            "probably": 0.6,
            "might": 0.4,
            "unsure": 0.2,
            "not sure": 0.2
        }
        
        text_lower = text.lower()
        for indicator, score in confidence_indicators.items():
            if indicator in text_lower:
                return score
        
        return 0.5  # é»˜è®¤ç½®ä¿¡åº¦

# æ³¨å†Œåˆ°å·¥å‚å‡½æ•°
def make_agent(agent_type: str):
    if agent_type == "my_custom":
        return MyCustomAgent
    # ... å…¶ä»– agent ç±»å‹
```

#### æ–¹æ¡ˆ B: ç»„åˆå¤šä¸ª Agent

```python
# æ–‡ä»¶: agent/ensemble_agent.py

class EnsembleAgent:
    """é›†æˆå¤šä¸ª Agent çš„å†³ç­–"""
    
    def __init__(self, prompt: str):
        self.agents = [
            AsearcherAgent(prompt),
            AsearcherReasoningAgent(prompt),
            MyCustomAgent(prompt)
        ]
        self.voting_history = []
    
    def prepare_llm_query(self) -> Tuple[str, Dict]:
        """å¤šæ•°æŠ•ç¥¨å†³å®šæŸ¥è¯¢"""
        proposals = []
        for agent in self.agents:
            prompt, params = agent.prepare_llm_query()
            proposals.append((prompt, params))
        
        # é€‰æ‹©æœ€å¸¸è§çš„æ–¹æ¡ˆæˆ–åŠ æƒç»„åˆ
        return self.vote_on_proposal(proposals)
    
    def vote_on_proposal(self, proposals):
        """æŠ•ç¥¨æœºåˆ¶"""
        # å®ç°æŠ•ç¥¨é€»è¾‘
        pass
```

### 6.2 æ·»åŠ æ–°å·¥å…·

#### ç¤ºä¾‹ 1: è®¡ç®—å™¨å·¥å…·

```python
# æ–‡ä»¶: tools/calculator_tool.py

import asyncio
from sympy import sympify, simplify, solve
from typing import Dict, Any

class CalculatorTool:
    """æ•°å­¦è®¡ç®—å·¥å…·"""
    
    async def calculate(self, expression: str) -> Dict[str, Any]:
        """æ‰§è¡Œæ•°å­¦è®¡ç®—"""
        try:
            # 1. è§£æè¡¨è¾¾å¼
            expr = sympify(expression)
            
            # 2. ç®€åŒ–è¡¨è¾¾å¼
            simplified = simplify(expr)
            
            # 3. æ•°å€¼è®¡ç®—
            result = float(simplified.evalf())
            
            return {
                "type": "calculation",
                "expression": expression,
                "simplified": str(simplified),
                "result": result,
                "success": True
            }
        except Exception as e:
            return {
                "type": "calculation",
                "expression": expression,
                "error": str(e),
                "success": False
            }
    
    async def solve_equation(self, equation: str, variable: str = "x") -> Dict[str, Any]:
        """æ±‚è§£æ–¹ç¨‹"""
        try:
            expr = sympify(equation)
            solutions = solve(expr, variable)
            
            return {
                "type": "equation_solving",
                "equation": equation,
                "variable": variable,
                "solutions": [str(s) for s in solutions],
                "success": True
            }
        except Exception as e:
            return {
                "type": "equation_solving",
                "error": str(e),
                "success": False
            }

# é›†æˆåˆ° SearchToolBox
# ä¿®æ”¹: AReaL/ASearcher/utils/search_tool.py

class SearchToolBox:
    def __init__(self, ...):
        super().__init__(...)
        self.calculator = CalculatorTool()
    
    async def step(self, qid_actions):
        # ... åŸæœ‰ä»£ç 
        
        # å¤„ç†è®¡ç®—å·¥å…·
        elif "<calculate>" in action and "</calculate>" in action:
            expr = action.split("<calculate>")[1].split("</calculate>")[0].strip()
            result = await self.calculator.calculate(expr)
            
            if result["success"]:
                result_text = f"Calculation result: {result['result']}"
            else:
                result_text = f"Calculation error: {result['error']}"
            
            return [{
                "type": "calculation",
                "text": result_text,
                "metadata": result
            }]
        
        # å¤„ç†æ–¹ç¨‹æ±‚è§£
        elif "<solve>" in action and "</solve>" in action:
            equation = action.split("<solve>")[1].split("</solve>")[0].strip()
            result = await self.calculator.solve_equation(equation)
            
            if result["success"]:
                result_text = f"Solutions: {', '.join(result['solutions'])}"
            else:
                result_text = f"Solving error: {result['error']}"
            
            return [{
                "type": "equation_solving",
                "text": result_text,
                "metadata": result
            }]
```

#### ç¤ºä¾‹ 2: ä»£ç æ‰§è¡Œå·¥å…·

```python
# æ–‡ä»¶: tools/code_executor.py

import asyncio
import subprocess
import tempfile
import os
from typing import Dict, Any

class CodeExecutor:
    """å®‰å…¨çš„ä»£ç æ‰§è¡Œå·¥å…·"""
    
    def __init__(self, timeout: int = 10, max_output_size: int = 10000):
        self.timeout = timeout
        self.max_output_size = max_output_size
        self.supported_languages = {
            "python": {"ext": ".py", "cmd": ["python3"]},
            "javascript": {"ext": ".js", "cmd": ["node"]},
            "bash": {"ext": ".sh", "cmd": ["bash"]}
        }
    
    async def execute(self, code: str, language: str = "python") -> Dict[str, Any]:
        """æ‰§è¡Œä»£ç å¹¶è¿”å›ç»“æœ"""
        if language not in self.supported_languages:
            return {
                "type": "code_execution",
                "success": False,
                "error": f"Unsupported language: {language}"
            }
        
        lang_config = self.supported_languages[language]
        
        try:
            # 1. åˆ›å»ºä¸´æ—¶æ–‡ä»¶
            with tempfile.NamedTemporaryFile(
                mode='w',
                suffix=lang_config["ext"],
                delete=False
            ) as f:
                f.write(code)
                temp_file = f.name
            
            # 2. æ‰§è¡Œä»£ç 
            process = await asyncio.create_subprocess_exec(
                *lang_config["cmd"],
                temp_file,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )
            
            # 3. ç­‰å¾…æ‰§è¡Œå®Œæˆï¼ˆå¸¦è¶…æ—¶ï¼‰
            try:
                stdout, stderr = await asyncio.wait_for(
                    process.communicate(),
                    timeout=self.timeout
                )
            except asyncio.TimeoutError:
                process.kill()
                return {
                    "type": "code_execution",
                    "success": False,
                    "error": f"Execution timeout ({self.timeout}s)"
                }
            
            # 4. å¤„ç†è¾“å‡º
            stdout_text = stdout.decode('utf-8')[:self.max_output_size]
            stderr_text = stderr.decode('utf-8')[:self.max_output_size]
            
            return {
                "type": "code_execution",
                "language": language,
                "code": code,
                "stdout": stdout_text,
                "stderr": stderr_text,
                "return_code": process.returncode,
                "success": process.returncode == 0
            }
            
        finally:
            # 5. æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if 'temp_file' in locals():
                os.unlink(temp_file)
```

### 6.3 è‡ªå®šä¹‰å¥–åŠ±å‡½æ•°

```python
# æ–‡ä»¶: AReaL/ASearcher/utils/rewards.py

def custom_composite_reward(action: str, ground_truth: str, 
                           trajectory: List[Dict], stats: Dict) -> float:
    """
    å¤åˆå¥–åŠ±å‡½æ•°ï¼šç»¼åˆè€ƒè™‘å¤šä¸ªå› ç´ 
    
    å› ç´ :
    1. ç­”æ¡ˆæ­£ç¡®æ€§ (F1/EM)
    2. æœç´¢æ•ˆç‡
    3. å“åº”è´¨é‡
    4. æ¨ç†åˆç†æ€§
    """
    
    # 1. åŸºç¡€æ­£ç¡®æ€§å¥–åŠ±
    _, f1_score = compute_score_f1(action, ground_truth)
    _, em_score = compute_score_em(action, ground_truth)
    correctness_reward = 0.7 * f1_score + 0.3 * em_score
    
    # 2. æ•ˆç‡å¥–åŠ±ï¼ˆæœç´¢æ¬¡æ•°å’Œè½®æ•°ï¼‰
    search_count = action.count("<search>")
    access_count = action.count("<access>")
    total_turns = stats.get("num_turns", 1)
    
    # ç†æƒ³æƒ…å†µï¼š2-3æ¬¡æœç´¢ï¼Œ1-2æ¬¡è®¿é—®ï¼Œ5-10è½®å¯¹è¯
    search_efficiency = max(0, 1 - abs(search_count - 2.5) * 0.2)
    access_efficiency = max(0, 1 - abs(access_count - 1.5) * 0.3)
    turn_efficiency = max(0, 1 - max(0, total_turns - 10) * 0.05)
    
    efficiency_reward = (search_efficiency + access_efficiency + turn_efficiency) / 3
    
    # 3. ç­”æ¡ˆè´¨é‡å¥–åŠ±
    answer_quality = evaluate_answer_quality(action)
    
    # 4. æ¨ç†é“¾å¥–åŠ±
    reasoning_score = evaluate_reasoning_chain(trajectory)
    
    # 5. ç»„åˆå¥–åŠ±ï¼ˆå¯å­¦ä¹ çš„æƒé‡ï¼‰
    weights = {
        "correctness": 0.5,
        "efficiency": 0.2,
        "quality": 0.15,
        "reasoning": 0.15
    }
    
    final_reward = (
        weights["correctness"] * correctness_reward +
        weights["efficiency"] * efficiency_reward +
        weights["quality"] * answer_quality +
        weights["reasoning"] * reasoning_score
    )
    
    # 6. é¢å¤–å¥–åŠ±/æƒ©ç½š
    # å¥–åŠ±æ‰¾åˆ°ç­”æ¡ˆ
    if "<answer>" in action:
        final_reward += 0.1
    
    # æƒ©ç½šè¿‡é•¿çš„å›ç­”
    if len(action) > 5000:
        final_reward -= 0.1
    
    # æƒ©ç½šé‡å¤æœç´¢
    if has_duplicate_searches(trajectory):
        final_reward -= 0.15
    
    return max(0, min(1, final_reward))  # è£å‰ªåˆ° [0, 1]

def evaluate_answer_quality(action: str) -> float:
    """è¯„ä¼°ç­”æ¡ˆè´¨é‡"""
    quality_score = 1.0
    
    # æ£€æŸ¥ç­”æ¡ˆæ˜¯å¦å­˜åœ¨
    if "<answer>" not in action:
        return 0.0
    
    answer = action.split("<answer>")[1].split("</answer>")[0]
    
    # é•¿åº¦åˆç†æ€§ï¼ˆ100-500 å­—ç¬¦æœ€ä½³ï¼‰
    length = len(answer)
    if length < 50:
        quality_score *= 0.5  # å¤ªçŸ­
    elif length > 1000:
        quality_score *= 0.7  # å¤ªé•¿
    
    # ç»“æ„å®Œæ•´æ€§
    if answer.strip().endswith(('.', '!', '?')):
        quality_score *= 1.1  # æœ‰ç»“æŸæ ‡ç‚¹
    
    # åŒ…å«æ•°å­—/äº‹å®
    import re
    if re.search(r'\d+', answer):
        quality_score *= 1.05  # åŒ…å«å…·ä½“æ•°å­—
    
    return min(1.0, quality_score)

def evaluate_reasoning_chain(trajectory: List[Dict]) -> float:
    """è¯„ä¼°æ¨ç†é“¾è´¨é‡"""
    reasoning_score = 1.0
    
    # æ£€æŸ¥æ˜¯å¦æœ‰é€»è¾‘é€’è¿›
    has_progression = check_logical_progression(trajectory)
    if has_progression:
        reasoning_score *= 1.2
    
    # æ£€æŸ¥æ˜¯å¦æœ‰æ•ˆåˆ©ç”¨æœç´¢ç»“æœ
    utilization_rate = calculate_info_utilization(trajectory)
    reasoning_score *= utilization_rate
    
    return min(1.0, reasoning_score)

def has_duplicate_searches(trajectory: List[Dict]) -> bool:
    """æ£€æŸ¥æ˜¯å¦æœ‰é‡å¤æœç´¢"""
    searches = []
    for record in trajectory:
        if record.get("type") == "search":
            query = record.get("query", "").lower().strip()
            if query in searches:
                return True
            searches.append(query)
    return False
```

### 6.4 è‡ªå®šä¹‰è®­ç»ƒæµç¨‹

```python
# æ–‡ä»¶: AReaL/ASearcher/train/custom_workflow.py

from AReaL.ASearcher.train.asearcher import ASearcherWorkflow
import numpy as np
import random

class CustomTrainingWorkflow(ASearcherWorkflow):
    """è‡ªå®šä¹‰è®­ç»ƒå·¥ä½œæµï¼šè¯¾ç¨‹å­¦ä¹  + æ•°æ®å¢å¼º"""
    
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        
        # è¯¾ç¨‹å­¦ä¹ å‚æ•°
        self.curriculum_enabled = kwargs.get("curriculum_enabled", True)
        self.difficulty_level = 0
        self.max_difficulty = 5
        self.difficulty_schedule = "linear"  # linear, exponential, adaptive
        
        # æ•°æ®å¢å¼ºå‚æ•°
        self.augmentation_enabled = kwargs.get("augmentation_enabled", True)
        self.augmentation_prob = 0.3
        
        # è‡ªé€‚åº”å‚æ•°
        self.success_rate_history = []
        self.adjust_interval = 100  # æ¯ 100 ä¸ª episode è°ƒæ•´ä¸€æ¬¡
        
    async def collect_agent_trajectory(self, valid_inst, qid, prompt, engine):
        """å¢å¼ºçš„è½¨è¿¹æ”¶é›†"""
        
        # 1. åº”ç”¨è¯¾ç¨‹å­¦ä¹ 
        if self.curriculum_enabled:
            prompt, max_turns = self.apply_curriculum(prompt)
            self.max_turns = max_turns
        
        # 2. åº”ç”¨æ•°æ®å¢å¼º
        if self.augmentation_enabled and random.random() < self.augmentation_prob:
            prompt = self.augment_prompt(prompt)
        
        # 3. æ”¶é›†è½¨è¿¹
        trajectory = await super().collect_agent_trajectory(
            valid_inst, qid, prompt, engine
        )
        
        # 4. è®°å½•é¢å¤–ä¿¡æ¯
        ground_truth, score, memory, stats = trajectory
        stats["difficulty_level"] = self.difficulty_level
        stats["augmented"] = self.augmentation_enabled
        
        return ground_truth, score, memory, stats
    
    def apply_curriculum(self, prompt: str) -> Tuple[str, int]:
        """åº”ç”¨è¯¾ç¨‹å­¦ä¹ ç­–ç•¥"""
        
        # æ ¹æ®éš¾åº¦çº§åˆ«è°ƒæ•´
        if self.difficulty_level == 0:
            # åˆçº§ï¼šç®€å•é—®é¢˜ï¼Œæ›´å¤šæç¤º
            prompt = f"""
            This is a simple question. Think step by step.
            
            {prompt}
            
            Hint: Start with a basic search about the main topic.
            """
            max_turns = 32
            
        elif self.difficulty_level <= 2:
            # ä¸­çº§ï¼šæ ‡å‡†é—®é¢˜ï¼Œé€‚åº¦æç¤º
            prompt = f"""
            {prompt}
            
            Remember to verify your findings with multiple sources.
            """
            max_turns = 64
            
        else:
            # é«˜çº§ï¼šå¤æ‚é—®é¢˜ï¼Œæ— æç¤º
            max_turns = 128
        
        return prompt, max_turns
    
    def augment_prompt(self, prompt: str) -> str:
        """æ•°æ®å¢å¼ºï¼šä¿®æ”¹ prompt"""
        augmentation_strategies = [
            self.paraphrase_question,
            self.add_noise,
            self.change_perspective,
            self.add_constraints
        ]
        
        strategy = random.choice(augmentation_strategies)
        return strategy(prompt)
    
    def paraphrase_question(self, prompt: str) -> str:
        """æ”¹å†™é—®é¢˜"""
        paraphrases = [
            "In other words, ",
            "To put it differently, ",
            "Rephrased: ",
            "Alternative formulation: "
        ]
        prefix = random.choice(paraphrases)
        return prefix + prompt
    
    def add_noise(self, prompt: str) -> str:
        """æ·»åŠ å™ªå£°ï¼ˆæ‹¼å†™é”™è¯¯ç­‰ï¼‰"""
        # éšæœºæ›¿æ¢ä¸€äº›å­—ç¬¦
        chars = list(prompt)
        n_changes = int(len(chars) * 0.02)  # 2% çš„å­—ç¬¦
        
        for _ in range(n_changes):
            idx = random.randint(0, len(chars) - 1)
            if chars[idx].isalpha():
                chars[idx] = random.choice('abcdefghijklmnopqrstuvwxyz')
        
        return ''.join(chars)
    
    def change_perspective(self, prompt: str) -> str:
        """æ”¹å˜è§†è§’"""
        perspectives = [
            "From an expert's perspective: ",
            "For a beginner: ",
            "Considering all factors: ",
            "From a critical viewpoint: "
        ]
        return random.choice(perspectives) + prompt
    
    def add_constraints(self, prompt: str) -> str:
        """æ·»åŠ çº¦æŸæ¡ä»¶"""
        constraints = [
            "\n\nPlease provide a concise answer.",
            "\n\nBe as detailed as possible.",
            "\n\nFocus on the most recent information.",
            "\n\nConsider multiple perspectives."
        ]
        return prompt + random.choice(constraints)
    
    async def arun_episode(self, engine, data):
        """è¿è¡Œè®­ç»ƒå›åˆï¼ˆå¸¦è‡ªé€‚åº”è°ƒæ•´ï¼‰"""
        
        # 1. æ‰§è¡ŒåŸæœ‰æµç¨‹
        results = await super().arun_episode(engine, data)
        
        # 2. è®°å½•æˆåŠŸç‡
        if results is not None:
            scores = [r.get("score", 0) for r in results]
            success_rate = sum(s > 0.5 for s in scores) / len(scores)
            self.success_rate_history.append(success_rate)
        
        # 3. è‡ªé€‚åº”è°ƒæ•´éš¾åº¦
        if len(self.success_rate_history) >= self.adjust_interval:
            self.adjust_difficulty()
            self.success_rate_history = []
        
        return results
    
    def adjust_difficulty(self):
        """æ ¹æ®æˆåŠŸç‡è°ƒæ•´éš¾åº¦"""
        avg_success_rate = np.mean(self.success_rate_history)
        
        if self.difficulty_schedule == "adaptive":
            # æˆåŠŸç‡é«˜äº 80%ï¼Œå¢åŠ éš¾åº¦
            if avg_success_rate > 0.8 and self.difficulty_level < self.max_difficulty:
                self.difficulty_level += 1
                logger.info(f"Increasing difficulty to {self.difficulty_level}")
                
            # æˆåŠŸç‡ä½äº 40%ï¼Œé™ä½éš¾åº¦
            elif avg_success_rate < 0.4 and self.difficulty_level > 0:
                self.difficulty_level -= 1
                logger.info(f"Decreasing difficulty to {self.difficulty_level}")
                
        elif self.difficulty_schedule == "linear":
            # çº¿æ€§å¢åŠ éš¾åº¦
            self.difficulty_level = min(
                self.max_difficulty,
                self.difficulty_level + 1
            )
            
        elif self.difficulty_schedule == "exponential":
            # æŒ‡æ•°å¢åŠ éš¾åº¦
            self.difficulty_level = min(
                self.max_difficulty,
                int(self.difficulty_level * 1.5 + 1)
            )

# é…ç½®æ–‡ä»¶: AReaL/ASearcher/configs/custom_training.yaml
workflow:
  _class: CustomTrainingWorkflow
  curriculum_enabled: true
  difficulty_schedule: adaptive
  augmentation_enabled: true
  augmentation_prob: 0.3
  max_turns: 128
  n_trajs: 2
  reward_type: F1
  
training:
  batch_size: 32
  learning_rate: 1e-5
  ppo_epochs: 4
  gradient_accumulation_steps: 2
  
evaluation:
  eval_interval: 500
  eval_samples: 100
```

### 6.5 å¿«é€Ÿå®éªŒå’Œæµ‹è¯•

```bash
# 1. æµ‹è¯•è‡ªå®šä¹‰ Agent
echo "æµ‹è¯•è‡ªå®šä¹‰ Agent..."
python evaluation/search_eval_async.py \
    --agent-type my_custom \
    --num_test_sample 10 \
    --data_names GAIA \
    --debug \
    --verbose

# 2. æµ‹è¯•æ–°å·¥å…·
echo "æµ‹è¯•è®¡ç®—å™¨å·¥å…·..."
python -m pytest tests/test_calculator_tool.py -v

# 3. éªŒè¯å¥–åŠ±å‡½æ•°
echo "éªŒè¯è‡ªå®šä¹‰å¥–åŠ±å‡½æ•°..."
cat > test_reward.py << 'EOF'
from AReaL.ASearcher.utils.rewards import custom_composite_reward

# æµ‹è¯•ç”¨ä¾‹
test_cases = [
    {
        "action": "<search>Python tutorial</search>\n<answer>Python is a programming language</answer>",
        "ground_truth": "Python is a high-level programming language",
        "expected_range": (0.6, 0.8)
    }
]

for i, case in enumerate(test_cases):
    reward = custom_composite_reward(
        case["action"], 
        case["ground_truth"],
        [], {}
    )
    print(f"Test {i+1}: Reward = {reward:.3f}")
    assert case["expected_range"][0] <= reward <= case["expected_range"][1]
    
print("All tests passed!")
EOF
python test_reward.py

# 4. å°è§„æ¨¡è®­ç»ƒæµ‹è¯•
echo "è¿è¡Œå°è§„æ¨¡è®­ç»ƒæµ‹è¯•..."
python -m areal.launcher.local \
    ASearcher/train/custom_workflow.py \
    --config ASearcher/configs/custom_training.yaml \
    --debug_mode \
    --max_steps 100 \
    --eval_interval 20
```

---

## 7. å­¦ä¹ èµ„æºä¸å®è·µ

### 7.1 è°ƒè¯•å’Œåˆ†ææŠ€å·§

```python
# è°ƒè¯•å·¥å…·é›†åˆ
# debug_utils.py

import time
import logging
import functools
from typing import Any, Callable
import traceback
import cProfile
import pstats
import io

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

def debug_trace(func: Callable) -> Callable:
    """å‡½æ•°è°ƒç”¨è¿½è¸ªè£…é¥°å™¨"""
    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        logger = logging.getLogger(func.__module__)
        logger.debug(f"Entering {func.__name__} with args={args[:2]}...")
        
        start_time = time.time()
        try:
            result = func(*args, **kwargs)
            elapsed = time.time() - start_time
            logger.debug(f"Exiting {func.__name__} (took {elapsed:.3f}s)")
            return result
        except Exception as e:
            logger.error(f"Error in {func.__name__}: {e}")
            logger.error(traceback.format_exc())
            raise
    
    return wrapper

def profile_function(func: Callable) -> Callable:
    """æ€§èƒ½åˆ†æè£…é¥°å™¨"""
    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        profiler = cProfile.Profile()
        profiler.enable()
        
        result = func(*args, **kwargs)
        
        profiler.disable()
        
        # æ‰“å°æ€§èƒ½æŠ¥å‘Š
        s = io.StringIO()
        ps = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        ps.print_stats(10)  # æ‰“å°å‰ 10 ä¸ªæœ€è€—æ—¶çš„å‡½æ•°
        
        print(f"\nProfile for {func.__name__}:")
        print(s.getvalue())
        
        return result
    
    return wrapper

class MemoryTracker:
    """å†…å­˜ä½¿ç”¨è¿½è¸ª"""
    def __init__(self):
        import tracemalloc
        tracemalloc.start()
        self.baseline = None
    
    def snapshot(self, label: str):
        import tracemalloc
        current, peak = tracemalloc.get_traced_memory()
        print(f"[{label}] Current: {current / 1024 / 1024:.2f}MB, "
              f"Peak: {peak / 1024 / 1024:.2f}MB")
        
        if self.baseline:
            diff = current - self.baseline
            print(f"  Diff from baseline: {diff / 1024 / 1024:+.2f}MB")
        else:
            self.baseline = current

# ä½¿ç”¨ç¤ºä¾‹
@debug_trace
@profile_function
def example_function(data):
    # å‡½æ•°å®ç°
    pass
```

### 7.2 æ€§èƒ½ä¼˜åŒ–å»ºè®®

```python
# æ€§èƒ½ä¼˜åŒ–æ£€æŸ¥æ¸…å•

1. å¼‚æ­¥ä¼˜åŒ–
   - ä½¿ç”¨ asyncio.gather() å¹¶å‘æ‰§è¡Œå¤šä¸ªä»»åŠ¡
   - é¿å…ä¸å¿…è¦çš„ awaitï¼Œæ‰¹é‡å¤„ç†
   - ä½¿ç”¨ asyncio.Queue è¿›è¡Œç”Ÿäº§è€…-æ¶ˆè´¹è€…æ¨¡å¼

2. å†…å­˜ä¼˜åŒ–
   - åŠæ—¶é‡Šæ”¾å¤§å¯¹è±¡ï¼šdel large_object
   - ä½¿ç”¨ç”Ÿæˆå™¨ä»£æ›¿åˆ—è¡¨ï¼šyield è€Œä¸æ˜¯ return []
   - æ§åˆ¶è½¨è¿¹ç¼“å­˜å¤§å°

3. GPU ä¼˜åŒ–
   - æ‰¹å¤„ç†æ¨ç†è¯·æ±‚
   - ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ
   - æ¢¯åº¦ç´¯ç§¯å‡å°‘æ˜¾å­˜ä½¿ç”¨

4. ç½‘ç»œä¼˜åŒ–
   - ä½¿ç”¨è¿æ¥æ± å¤ç”¨ HTTP è¿æ¥
   - å®ç°è¯·æ±‚ç¼“å­˜æœºåˆ¶
   - æ‰¹é‡è¯·æ±‚åˆå¹¶

# ç¤ºä¾‹ï¼šæ‰¹å¤„ç†ä¼˜åŒ–
class BatchedInference:
    def __init__(self, batch_size=8, timeout=0.1):
        self.batch_size = batch_size
        self.timeout = timeout
        self.queue = asyncio.Queue()
        
    async def add_request(self, request):
        future = asyncio.Future()
        await self.queue.put((request, future))
        return await future
    
    async def process_batch(self):
        batch = []
        futures = []
        
        # æ”¶é›†æ‰¹æ¬¡
        deadline = time.time() + self.timeout
        while len(batch) < self.batch_size and time.time() < deadline:
            try:
                timeout_remaining = deadline - time.time()
                request, future = await asyncio.wait_for(
                    self.queue.get(),
                    timeout=max(0.01, timeout_remaining)
                )
                batch.append(request)
                futures.append(future)
            except asyncio.TimeoutError:
                break
        
        if batch:
            # æ‰¹å¤„ç†
            results = await self.batch_inference(batch)
            
            # è¿”å›ç»“æœ
            for future, result in zip(futures, results):
                future.set_result(result)
```

### 7.3 å¸¸è§é—®é¢˜ä¸è§£å†³æ–¹æ¡ˆ

```bash
# é—®é¢˜è¯Šæ–­è„šæœ¬
# diagnose.sh

#!/bin/bash

echo "=== ASearcher ç¯å¢ƒè¯Šæ–­ ==="

# 1. æ£€æŸ¥ Python ç‰ˆæœ¬
echo -n "Python ç‰ˆæœ¬: "
python3 --version

# 2. æ£€æŸ¥ä¾èµ–
echo -n "PyTorch ç‰ˆæœ¬: "
python3 -c "import torch; print(torch.__version__)"

echo -n "Transformers ç‰ˆæœ¬: "
python3 -c "import transformers; print(transformers.__version__)"

# 3. æ£€æŸ¥ GPU
echo "GPU ä¿¡æ¯:"
python3 -c "import torch; print(f'CUDA available: {torch.cuda.is_available()}'); print(f'GPU count: {torch.cuda.device_count()}')"

# 4. æ£€æŸ¥ API Keys
echo "API Keys é…ç½®:"
[ -z "$SERPER_API_KEY" ] && echo "  âŒ SERPER_API_KEY æœªè®¾ç½®" || echo "  âœ… SERPER_API_KEY å·²è®¾ç½®"
[ -z "$JINA_API_KEY" ] && echo "  âŒ JINA_API_KEY æœªè®¾ç½®" || echo "  âœ… JINA_API_KEY å·²è®¾ç½®"

# 5. æ£€æŸ¥ç½‘ç»œè¿æ¥
echo "ç½‘ç»œè¿æ¥æµ‹è¯•:"
curl -s -o /dev/null -w "  Serper API: %{http_code}\n" https://api.serper.dev/
curl -s -o /dev/null -w "  Jina API: %{http_code}\n" https://api.jina.ai/

# 6. æ£€æŸ¥ç£ç›˜ç©ºé—´
echo "ç£ç›˜ç©ºé—´:"
df -h . | tail -1 | awk '{print "  å¯ç”¨ç©ºé—´: " $4}'

echo "=== è¯Šæ–­å®Œæˆ ==="
```

### 7.4 å­¦ä¹ æ£€æŸ¥ç‚¹

å®Œæˆæœ¬æŒ‡å—å­¦ä¹ åï¼Œä½ åº”è¯¥èƒ½å¤Ÿï¼š

#### ç†è§£å±‚é¢
- [ ] è§£é‡Šå¼‚æ­¥ RL å¦‚ä½•è§£å†³é•¿è½¨è¿¹è®­ç»ƒçš„ GPU ç©ºé—²é—®é¢˜
- [ ] è¯´æ˜ Agent Memory å’Œ job_queue çš„ä½œç”¨æœºåˆ¶
- [ ] æè¿°å·¥å…·è°ƒç”¨çš„ XML æ ‡ç­¾è§¦å‘æœºåˆ¶
- [ ] ç†è§£ PPO è®­ç»ƒä¸­çš„ä¼˜åŠ¿å‡½æ•°è®¡ç®—
- [ ] è§£é‡Š RID ç¼“å­˜å¦‚ä½•ä¿è¯è½¨è¿¹è·¯ç”±ä¸€è‡´æ€§

#### å®è·µå±‚é¢
- [ ] æˆåŠŸè¿è¡Œè¯„ä¼°è„šæœ¬å¹¶ç†è§£è¾“å‡º
- [ ] èƒ½å¤Ÿè¿½è¸ªä¸€ä¸ªé—®é¢˜ä»è¾“å…¥åˆ°ç­”æ¡ˆçš„å®Œæ•´æµç¨‹
- [ ] å®šä½ä»»ä½•åŠŸèƒ½åœ¨ä»£ç ä¸­çš„å®ç°ä½ç½®
- [ ] ä¿®æ”¹ Agent è¡Œä¸ºï¼ˆå¦‚è°ƒæ•´æç¤ºè¯æˆ–é‡‡æ ·å‚æ•°ï¼‰
- [ ] æ·»åŠ æ–°çš„å·¥å…·ç±»å‹
- [ ] å®ç°è‡ªå®šä¹‰å¥–åŠ±å‡½æ•°
- [ ] ç†è§£å¹¶ä¿®æ”¹è®­ç»ƒé…ç½®

#### è¿›é˜¶å±‚é¢
- [ ] å®ç°è¯¾ç¨‹å­¦ä¹ ç­–ç•¥
- [ ] ä¼˜åŒ–å¼‚æ­¥å¹¶å‘å‚æ•°
- [ ] åˆ†æå’Œè§£å†³æ€§èƒ½ç“¶é¢ˆ
- [ ] è´¡çŒ®ä»£ç åˆ°ä¸»ä»“åº“

### 7.5 æ¨èå­¦ä¹ è·¯å¾„

```mermaid
graph TB
    A[Day 1-2: ç¯å¢ƒæ­å»º] --> B[Day 3-4: è¿è¡Œè¯„ä¼°]
    B --> C[Day 5-6: ç†è§£ Agent Core]
    C --> D[Day 7-8: ç†è§£å·¥å…·è°ƒç”¨]
    D --> E[Day 9-10: ç†è§£è¯„ä¼°æµç¨‹]
    E --> F[Day 11-13: ç†è§£è®­ç»ƒæ¡†æ¶]
    F --> G[Day 14-15: è‡ªå®šä¹‰ Agent]
    G --> H[Day 16-17: æ·»åŠ æ–°å·¥å…·]
    H --> I[Day 18-20: å®Œæ•´é¡¹ç›®å®è·µ]
    
    style A fill:#f9f,stroke:#333
    style I fill:#9f9,stroke:#333
```

### 7.6 è¿›ä¸€æ­¥å­¦ä¹ èµ„æº

1. **é¡¹ç›®èµ„æº**
   - [è®ºæ–‡](https://arxiv.org/abs/2508.07976): ç†è§£ç†è®ºåŸºç¡€
   - [æ¨¡å‹æƒé‡](https://huggingface.co/collections/inclusionAI/asearcher-6891d8acad5ebc3a1e1fb2d1): ä¸‹è½½é¢„è®­ç»ƒæ¨¡å‹
   - [æµ‹è¯•æ•°æ®](https://huggingface.co/datasets/inclusionAI/ASearcher-test-data): è¯„ä¼°æ•°æ®é›†
   - [AReaL æ–‡æ¡£](https://inclusionai.github.io/AReaL/): è®­ç»ƒæ¡†æ¶è¯¦ç»†æ–‡æ¡£

2. **ç›¸å…³é¡¹ç›®**
   - [Search-o1](https://search-o1.github.io/): æœç´¢ä¼˜åŒ–ç ”ç©¶
   - [Search-R1](https://github.com/PeterGriffinJin/Search-R1): å¦ä¸€ç§æœç´¢ Agent å®ç°
   - [WebAgent](https://github.com/Alibaba-NLP/WebAgent): ç½‘é¡µäº¤äº’ Agent

3. **ç¤¾åŒºèµ„æº**
   - GitHub Issues: é—®é¢˜è®¨è®ºå’Œ bug æŠ¥å‘Š
   - Discord/Slack: å®æ—¶äº¤æµï¼ˆå¦‚æœæœ‰ï¼‰
   - è´¡çŒ®æŒ‡å—: CONTRIBUTING.md

---

## æ€»ç»“

æœ¬å­¦ä¹ æŒ‡å—é€šè¿‡æ¨¡å—åŒ–çš„æ–¹å¼ç³»ç»Ÿåœ°ä»‹ç»äº† ASearcher é¡¹ç›®çš„æ¶æ„ã€å®ç°ç»†èŠ‚å’Œè‡ªå®šä¹‰å¼€å‘æ–¹æ³•ã€‚å…³é”®è¦ç‚¹ï¼š

1. **å¼‚æ­¥æ¶æ„æ˜¯æ ¸å¿ƒåˆ›æ–°**ï¼šå®Œå…¨è§£è€¦çš„è½¨è¿¹æ”¶é›†å’Œæ¨¡å‹è®­ç»ƒå°† GPU åˆ©ç”¨ç‡ä» 40-60% æå‡åˆ° 85-95%

2. **æ¨¡å—åŒ–è®¾è®¡ä¾¿äºæ‰©å±•**ï¼šAgent Coreã€Training Frameworkã€Evaluation System ä¸‰å¤§æ¨¡å—æ¾è€¦åˆï¼Œæ˜“äºç‹¬ç«‹å¼€å‘

3. **ç»Ÿä¸€çš„å·¥å…·è°ƒç”¨æ¥å£**ï¼šé€šè¿‡ XML æ ‡ç­¾å®ç°æ¸…æ™°çš„å·¥å…·è°ƒç”¨æœºåˆ¶

4. **ä¸°å¯Œçš„è‡ªå®šä¹‰å…¥å£**ï¼šä»ç®€å•çš„ Agent è¡Œä¸ºä¿®æ”¹åˆ°å¤æ‚çš„è®­ç»ƒæµç¨‹å®šåˆ¶ï¼Œéƒ½æœ‰æ¸…æ™°çš„æ‰©å±•ç‚¹

5. **å®è·µé©±åŠ¨çš„å­¦ä¹ è·¯å¾„**ï¼šé€šè¿‡å®é™…è¿è¡Œã€ä¿®æ”¹ã€æµ‹è¯•æ¥æ·±å…¥ç†è§£ç³»ç»Ÿ

å¸Œæœ›è¿™ä»½æŒ‡å—èƒ½å¸®åŠ©ä½ å¿«é€ŸæŒæ¡ ASearcher é¡¹ç›®ï¼Œå¹¶åŸºäºæ­¤å¼€å‘å‡ºæ›´å¼ºå¤§çš„æœç´¢æ™ºèƒ½ä½“ï¼

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0  
**æœ€åæ›´æ–°**: 2025å¹´1æœˆ  
**ä½œè€…**: Claude Code Assistant  
**è´¡çŒ®**: æ¬¢è¿æäº¤ PR æ”¹è¿›æœ¬æ–‡æ¡£

å¦‚æœ‰ä»»ä½•é—®é¢˜æˆ–å»ºè®®ï¼Œè¯·åœ¨ GitHub Issues ä¸­æå‡ºã€‚