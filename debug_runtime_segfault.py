#!/usr/bin/env python3
"""
è°ƒè¯•è¿è¡Œæ—¶æ®µé”™è¯¯ - é€æ­¥æ¨¡æ‹ŸSGLangå¯åŠ¨è¿‡ç¨‹
"""

import sys
import os
import traceback
import torch

def test_step_by_step_initialization():
    """é€æ­¥æµ‹è¯•SGLangåˆå§‹åŒ–è¿‡ç¨‹"""
    
    print("=" * 60)
    print("é€æ­¥æµ‹è¯•SGLangåˆå§‹åŒ–")
    print("=" * 60)
    
    model_path = "/mnt/dolphinfs/hdd_pool/docker/user/hadoop-mtsearch-assistant/ai-search/deepsearch_files/LLMbasemodels/huggingface.co/Qwen/Qwen3-8B"
    
    steps = [
        {
            "name": "1. å¯¼å…¥åŸºç¡€æ¨¡å—",
            "code": """
import sglang
import sglang.srt.model_executor.model_runner
from sglang.srt.layers.torchao_utils import apply_torchao_config_to_model
print("åŸºç¡€æ¨¡å—å¯¼å…¥æˆåŠŸ")
"""
        },
        {
            "name": "2. åˆ›å»ºå¯åŠ¨å‚æ•°",
            "code": f"""
from sglang.srt.server_args import ServerArgs
args = ServerArgs(
    model_path="{model_path}",
    tokenizer_path="{model_path}",
    host="localhost",
    port=12345,
    dtype="bfloat16",
    tp_size=1,
    context_length=4096,
    mem_fraction_static=0.5,
    attention_backend="fa3",
)
print("å¯åŠ¨å‚æ•°åˆ›å»ºæˆåŠŸ:", type(args))
"""
        },
        {
            "name": "3. æµ‹è¯•TorchAOé…ç½®åº”ç”¨",
            "code": """
# æ¨¡æ‹Ÿapply_torchao_configè°ƒç”¨
dummy_model = torch.nn.Linear(10, 10)
try:
    apply_torchao_config_to_model(dummy_model, "")  # ç©ºé…ç½®
    print("TorchAOé…ç½®åº”ç”¨æˆåŠŸ")
except Exception as e:
    print(f"TorchAOé…ç½®åº”ç”¨å¤±è´¥: {e}")
    if "torchao" in str(e).lower():
        raise e
"""
        },
        {
            "name": "4. æµ‹è¯•æ¨¡å‹åŠ è½½å™¨åˆ›å»º",
            "code": f"""
from sglang.srt.model_executor.model_runner import ModelRunner
from sglang.srt.server_args import ServerArgs

args = ServerArgs(
    model_path="{model_path}",
    tokenizer_path="{model_path}",
    dtype="bfloat16",
    tp_size=1,
    context_length=4096,
    mem_fraction_static=0.5,
)

# åªåˆ›å»ºModelRunnerå¯¹è±¡ï¼Œä¸åŠ è½½æ¨¡å‹
try:
    model_runner = ModelRunner(
        model_config=None,  # å…ˆç”¨Noneæµ‹è¯•
        mem_fraction_static=0.5,
        gpu_id=0,
        tp_rank=0,
        tp_size=1,
        nccl_port=None,
        server_args=args,
    )
    print("ModelRunneråˆ›å»ºæˆåŠŸ")
except Exception as e:
    print(f"ModelRunneråˆ›å»ºå¤±è´¥: {e}")
    raise e
"""
        },
        {
            "name": "5. æµ‹è¯•CUDAå†…å­˜åˆ†é…",
            "code": """
import torch
print(f"GPUå¯ç”¨: {torch.cuda.is_available()}")
print(f"GPUæ•°é‡: {torch.cuda.device_count()}")

# æµ‹è¯•åŸºæœ¬CUDAæ“ä½œ
try:
    x = torch.randn(100, 100).cuda()
    y = torch.matmul(x, x.t())
    print("åŸºæœ¬CUDAæ“ä½œæˆåŠŸ")
except Exception as e:
    print(f"CUDAæ“ä½œå¤±è´¥: {e}")
    raise e
"""
        },
        {
            "name": "6. æµ‹è¯•sgl_kernelå‡½æ•°è°ƒç”¨",
            "code": """
import sgl_kernel
# æµ‹è¯•sgl_kernelçš„ä¸€äº›åŸºæœ¬å‡½æ•°
try:
    # æ£€æŸ¥sgl_kernelæ˜¯å¦æœ‰å¸¸ç”¨çš„æ“ä½œå‡½æ•°
    attrs = [attr for attr in dir(sgl_kernel) if not attr.startswith('_')]
    print(f"sgl_kernelå¯ç”¨å‡½æ•°: {len(attrs)}ä¸ª")
    
    # å°è¯•è°ƒç”¨ä¸€ä¸ªç®€å•çš„å‡½æ•°ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    if hasattr(sgl_kernel, 'common_ops'):
        print("sgl_kernel.common_opså¯ç”¨")
        # ä¸å®é™…è°ƒç”¨ï¼Œé¿å…æ®µé”™è¯¯
    else:
        print("sgl_kernel.common_opsä¸å¯ç”¨")
        
except Exception as e:
    print(f"sgl_kernelæµ‹è¯•å¤±è´¥: {e}")
    raise e
"""
        }
    ]
    
    for step in steps:
        print(f"\n{step['name']}:")
        try:
            # åœ¨å­è¿›ç¨‹ä¸­æ‰§è¡Œï¼Œé¿å…æ®µé”™è¯¯å½±å“ä¸»è¿›ç¨‹
            import subprocess
            result = subprocess.run([
                sys.executable, "-c", step['code']
            ], capture_output=True, text=True, timeout=30)
            
            if result.returncode == 0:
                print(f"   âœ“ æˆåŠŸ")
                if result.stdout:
                    for line in result.stdout.strip().split('\n'):
                        print(f"     {line}")
            elif result.returncode == -11:
                print(f"   âœ— æ®µé”™è¯¯ - é—®é¢˜å‡ºç°åœ¨è¿™ä¸€æ­¥!")
                return step['name']
            else:
                print(f"   âœ— å¤±è´¥ (é€€å‡ºç : {result.returncode})")
                if result.stderr:
                    for line in result.stderr.strip().split('\n')[:3]:
                        print(f"     é”™è¯¯: {line}")
                        
        except subprocess.TimeoutExpired:
            print(f"   âœ— è¶…æ—¶")
        except Exception as e:
            print(f"   âœ— æ‰§è¡Œå¼‚å¸¸: {e}")
    
    return None

def test_torchao_specific_issue():
    """ä¸“é—¨æµ‹è¯•TorchAOç›¸å…³çš„é—®é¢˜"""
    
    print("\n" + "=" * 60)
    print("TorchAO ç‰¹å®šé—®é¢˜æµ‹è¯•")
    print("=" * 60)
    
    # æµ‹è¯•æœ‰æ— TorchAOæ—¶çš„å·®å¼‚
    test_cases = [
        {
            "name": "ç¦ç”¨TorchAOæ—¶çš„å¯åŠ¨",
            "env": {"SGLANG_DISABLE_TORCHAO": "1"},
            "args": ["--help"]
        },
        {
            "name": "å¯ç”¨TorchAOæ—¶çš„å¯åŠ¨", 
            "env": {},
            "args": ["--help"]
        },
        {
            "name": "æœ€å°å‚æ•°å¯åŠ¨æµ‹è¯•",
            "env": {},
            "args": [
                "--model-path", "/mnt/dolphinfs/hdd_pool/docker/user/hadoop-mtsearch-assistant/ai-search/deepsearch_files/LLMbasemodels/huggingface.co/Qwen/Qwen3-8B",
                "--host", "localhost",
                "--port", "12345",
                "--dtype", "bfloat16", 
                "--dry-run"  # å¦‚æœæ”¯æŒçš„è¯
            ]
        }
    ]
    
    for case in test_cases:
        print(f"\næµ‹è¯•: {case['name']}")
        
        cmd = [sys.executable, "-m", "sglang.launch_server"] + case['args']
        env = os.environ.copy()
        env.update(case.get('env', {}))
        
        try:
            import subprocess
            result = subprocess.run(
                cmd, 
                capture_output=True, 
                text=True, 
                timeout=20,
                env=env
            )
            
            if result.returncode == 0:
                print("   âœ“ æˆåŠŸ")
            elif result.returncode == -11:
                print("   âœ— æ®µé”™è¯¯")
            else:
                print(f"   ? é€€å‡ºç : {result.returncode}")
                
            if result.stderr and "torchao" in result.stderr.lower():
                print("   >> å‘ç°TorchAOç›¸å…³é”™è¯¯:")
                for line in result.stderr.split('\n')[:3]:
                    if line.strip():
                        print(f"      {line}")
                        
        except Exception as e:
            print(f"   æ‰§è¡Œå¼‚å¸¸: {e}")

def suggest_next_steps():
    """å»ºè®®ä¸‹ä¸€æ­¥è°ƒè¯•æ–¹æ¡ˆ"""
    
    print("\n" + "=" * 60)
    print("ä¸‹ä¸€æ­¥è°ƒè¯•å»ºè®®")
    print("=" * 60)
    
    suggestions = [
        "1. å¦‚æœæ®µé”™è¯¯å‡ºç°åœ¨æ¨¡å‹åŠ è½½é˜¶æ®µï¼Œå°è¯•:",
        "   - ä½¿ç”¨æ›´å°çš„æµ‹è¯•æ¨¡å‹",
        "   - å‡å°context_lengthå’Œmem_fraction_static",
        "",
        "2. å¦‚æœæ®µé”™è¯¯å‡ºç°åœ¨TorchAOé…ç½®é˜¶æ®µï¼Œå°è¯•:",
        "   - å®Œå…¨å¸è½½TorchAO: pip uninstall torchao -y",
        "   - è®¾ç½®ç¯å¢ƒå˜é‡: export SGLANG_DISABLE_TORCHAO=1",
        "",
        "3. å¦‚æœæ®µé”™è¯¯å‡ºç°åœ¨sgl_kernelè°ƒç”¨é˜¶æ®µï¼Œå°è¯•:",
        "   - é™çº§PyTorch: pip install torch==2.4.0",
        "   - æˆ–è€…æš‚æ—¶ç¦ç”¨sgl_kernelä¼˜åŒ–",
        "",
        "4. æœ€ç»ˆæ–¹æ¡ˆ - ä½¿ç”¨å·²çŸ¥ç¨³å®šçš„ç‰ˆæœ¬ç»„åˆ:",
        "   - PyTorch 2.4.0 + SGLang 0.4.8 + sgl-kernel 0.2.6"
    ]
    
    for suggestion in suggestions:
        print(suggestion)

if __name__ == "__main__":
    failed_step = test_step_by_step_initialization()
    test_torchao_specific_issue()
    suggest_next_steps()
    
    if failed_step:
        print(f"\nğŸ¯ å…³é”®å‘ç°: é—®é¢˜å‡ºç°åœ¨ '{failed_step}'")
    else:
        print(f"\nğŸ¤” å¥‡æ€ª: æ‰€æœ‰æ­¥éª¤éƒ½æˆåŠŸäº†ï¼Œä½†å®é™…å¯åŠ¨å¤±è´¥")
        print("è¿™å¯èƒ½æ˜¯å¤šçº¿ç¨‹æˆ–GPUåˆå§‹åŒ–ç›¸å…³çš„é—®é¢˜")
